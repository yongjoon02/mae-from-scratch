{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAE Downstream Tasks with PyTorch Lightning\n",
    "\n",
    "Three downstream tasks using pretrained MAE encoder:\n",
    "1. Classification - CIFAR-10\n",
    "2. Object Detection - Oxford-IIIT Pet\n",
    "3. Semantic Segmentation - Oxford-IIIT Pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpl\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TensorBoardLogger, CSVLogger\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\pytorch_lightning\\__init__.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m     _logger\u001b[38;5;241m.\u001b[39maddHandler(logging\u001b[38;5;241m.\u001b[39mStreamHandler())\n\u001b[0;32m     23\u001b[0m     _logger\u001b[38;5;241m.\u001b[39mpropagate \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_possible_user_warnings  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpytorch_lightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callback  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_fabric\\__init__.py:35\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mplatform \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwin32\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     32\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE_LIBUV\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfabric\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Fabric  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mseed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m seed_everything  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwarnings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m disable_possible_user_warnings  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_fabric\\fabric.py:38\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchSampler, DataLoader, DistributedSampler, RandomSampler, SequentialSampler\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _PLUGIN_INPUT, _PRECISION_INPUT, _Connector, _is_using_cli\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloggers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Logger\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_fabric\\accelerators\\__init__.py:15\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# you may not use this file except in compliance with the License.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpu\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CPUAccelerator  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CUDAAccelerator, find_usable_cuda_devices  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_fabric\\accelerators\\accelerator.py:19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _AcceleratorRegistry\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mAccelerator\u001b[39;00m(ABC):\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The Accelerator base class.\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    An Accelerator is meant to deal with one type of hardware.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_fabric\\accelerators\\registry.py:18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Optional\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MisconfigurationException\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _register_classes\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_AcceleratorRegistry\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_fabric\\utilities\\__init__.py:16\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright The Lightning AI team.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"General utilities.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply_func\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m move_data_to_device\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AttributeDict, suggested_max_num_workers\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_shared_filesystem\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_fabric\\utilities\\apply_func.py:24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_utilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply_func\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_to_collection\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Tensor\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _NUMPY_AVAILABLE\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning_fabric\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _DEVICE\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_fabric\\utilities\\imports.py:39\u001b[0m\n\u001b[0;32m     37\u001b[0m _TORCH_GREATER_EQUAL_2_5 \u001b[38;5;241m=\u001b[39m compare_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, operator\u001b[38;5;241m.\u001b[39mge, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.5.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m _TORCH_LESS_EQUAL_2_6 \u001b[38;5;241m=\u001b[39m compare_version(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, operator\u001b[38;5;241m.\u001b[39mle, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m _TORCHMETRICS_GREATER_EQUAL_1_0_0 \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_version\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtorchmetrics\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m1.0.0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m _PYTHON_GREATER_EQUAL_3_10_0 \u001b[38;5;241m=\u001b[39m (sys\u001b[38;5;241m.\u001b[39mversion_info\u001b[38;5;241m.\u001b[39mmajor, sys\u001b[38;5;241m.\u001b[39mversion_info\u001b[38;5;241m.\u001b[39mminor) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_utilities\\core\\imports.py:78\u001b[0m, in \u001b[0;36mcompare_version\u001b[1;34m(package, op, version, use_base_version)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compare package version with some requirements.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m>>> compare_version(\"torch\", operator.ge, \"0.1\")\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     75\u001b[0m \n\u001b[0;32m     76\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     pkg \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\torchmetrics\\__init__.py:37\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(scipy\u001b[38;5;241m.\u001b[39msignal, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhamming\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     35\u001b[0m         scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mhamming \u001b[38;5;241m=\u001b[39m scipy\u001b[38;5;241m.\u001b[39msignal\u001b[38;5;241m.\u001b[39mwindows\u001b[38;5;241m.\u001b[39mhamming\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m functional  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maggregation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     39\u001b[0m     CatMetric,\n\u001b[0;32m     40\u001b[0m     MaxMetric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m     SumMetric,\n\u001b[0;32m     46\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _PermutationInvariantTraining \u001b[38;5;28;01mas\u001b[39;00m PermutationInvariantTraining  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\torchmetrics\\functional\\__init__.py:55\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _signal_noise_ratio \u001b[38;5;28;01mas\u001b[39;00m signal_noise_ratio\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     25\u001b[0m     accuracy,\n\u001b[0;32m     26\u001b[0m     auroc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     53\u001b[0m     stat_scores,\n\u001b[0;32m     54\u001b[0m )\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _panoptic_quality \u001b[38;5;28;01mas\u001b[39;00m panoptic_quality\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     57\u001b[0m     _error_relative_global_dimensionless_synthesis \u001b[38;5;28;01mas\u001b[39;00m error_relative_global_dimensionless_synthesis,\n\u001b[0;32m     58\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecated\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _image_gradients \u001b[38;5;28;01mas\u001b[39;00m image_gradients\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\torchmetrics\\functional\\detection\\__init__.py:22\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutilities\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     _TORCHVISION_AVAILABLE,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     20\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodified_panoptic_quality\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpanoptic_quality\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TORCHVISION_AVAILABLE:\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mciou\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m complete_intersection_over_union\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdetection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiou\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_intersection_over_union\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_utilities\\core\\imports.py:200\u001b[0m, in \u001b[0;36mRequirementCache.__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format as bool.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_utilities\\core\\imports.py:166\u001b[0m, in \u001b[0;36mRequirementCache._check_available\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequirement:\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_requirement\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavailable\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_module()\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\lightning_utilities\\core\\imports.py:131\u001b[0m, in \u001b[0;36mRequirementCache._check_requirement\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    130\u001b[0m     req \u001b[38;5;241m=\u001b[39m Requirement(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequirement)\n\u001b[1;32m--> 131\u001b[0m     pkg_version \u001b[38;5;241m=\u001b[39m \u001b[43mVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavailable \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mspecifier\u001b[38;5;241m.\u001b[39mcontains(pkg_version, prereleases\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;129;01mnot\u001b[39;00m req\u001b[38;5;241m.\u001b[39mextras \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_extras_available(req)\n\u001b[0;32m    134\u001b[0m     )\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (PackageNotFoundError, InvalidVersion) \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "File \u001b[1;32mf:\\mae-from-scratch\\.venv\\lib\\site-packages\\packaging\\version.py:200\u001b[0m, in \u001b[0;36mVersion.__init__\u001b[1;34m(self, version)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a Version object.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m:param version:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    exception will be raised.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Validate the version and parse it into pieces\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_regex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidVersion(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from torchmetrics import Accuracy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd() if (Path.cwd() / \"checkpoints\").exists() else Path.cwd().parent\n",
    "WEIGHT_PATH = PROJECT_ROOT / \"checkpoints\" / \"mae_pretrain_vit_base.pth\"\n",
    "LOG_DIR = PROJECT_ROOT / \"logs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, int(dim * mlp_ratio))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MAEEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        class PatchEmbed(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "            def forward(self, x):\n",
    "                return self.proj(x)\n",
    "        \n",
    "        self.patch_embed = PatchEmbed()\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.blocks = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "def load_mae_encoder(weight_path):\n",
    "    model = MAEEncoder()\n",
    "    ckpt = torch.load(weight_path, map_location=\"cpu\", weights_only=False)\n",
    "    if 'model' in ckpt:\n",
    "        state_dict = ckpt['model']\n",
    "    elif 'state_dict' in ckpt:\n",
    "        state_dict = ckpt['state_dict']\n",
    "    else:\n",
    "        state_dict = ckpt\n",
    "    \n",
    "    for prefix in [\"module.\", \"model.\", \"net.\"]:\n",
    "        state_dict = {k[len(prefix):] if k.startswith(prefix) else k: v for k, v in state_dict.items()}\n",
    "    \n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "encoder = load_mae_encoder(WEIGHT_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Classification (CIFAR-10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# CIFAR-10 DataModule\n",
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        datasets.CIFAR10(self.data_dir, train=True, download=True)\n",
    "        datasets.CIFAR10(self.data_dir, train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            full_train = datasets.CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
    "            train_size = int(0.9 * len(full_train))\n",
    "            val_size = len(full_train) - train_size\n",
    "            self.train_dataset, self.val_dataset = random_split(full_train, [train_size, val_size])\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = datasets.CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=self.num_workers)\n",
    "\n",
    "# Classification Model\n",
    "class MAEClassifier(pl.LightningModule):\n",
    "    def __init__(self, encoder, num_classes=10, freeze_encoder=True, lr=1e-3, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['encoder'])\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.head = nn.Linear(768, num_classes)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        cls_token = features[:, 0]\n",
    "        return self.head(cls_token)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.train_acc(preds, y)\n",
    "        \n",
    "        self.log('train/loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train/acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val/loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val/acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.test_acc(preds, y)\n",
    "        \n",
    "        self.log('test/loss', loss)\n",
    "        self.log('test/acc', acc)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.head.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val/loss'\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification 학습\n",
    "data_module = CIFAR10DataModule(data_dir=str(PROJECT_ROOT / \"data\"), batch_size=32, num_workers=0)\n",
    "model = MAEClassifier(encoder=encoder, num_classes=10, freeze_encoder=True, lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "tensorboard_logger = TensorBoardLogger(LOG_DIR, name=\"mae_classification\")\n",
    "csv_logger = CSVLogger(LOG_DIR, name=\"mae_classification\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=LOG_DIR / \"checkpoints\",\n",
    "    filename=\"mae-{epoch:02d}-{val/acc:.4f}\",\n",
    "    monitor=\"val/acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=3\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor=\"val/loss\", patience=20, mode=\"min\", verbose=True)\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=[tensorboard_logger, csv_logger],\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Classification training...\")\n",
    "trainer.fit(model, data_module)\n",
    "print(\"Training completed!\")\n",
    "\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "trainer.validate(model, data_module)\n",
    "print(\"Validation completed!\")\n",
    "\n",
    "print(\"\\nEvaluating on test set...\")\n",
    "trainer.test(model, data_module)\n",
    "print(\"Test completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification 시각화\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "test_loader = data_module.test_dataloader()\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "sample_images, sample_preds, sample_labels = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y.numpy())\n",
    "        if i == 0:\n",
    "            sample_images = x[:16].cpu()\n",
    "            sample_preds = preds[:16]\n",
    "            sample_labels = y[:16].numpy()\n",
    "\n",
    "all_preds, all_labels = np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "fig.suptitle('Classification Test Results', fontsize=16, fontweight='bold')\n",
    "gs = fig.add_gridspec(3, 6, hspace=0.4, wspace=0.3)\n",
    "\n",
    "for idx in range(12):\n",
    "    ax = fig.add_subplot(gs[idx // 6, idx % 6])\n",
    "    img = sample_images[idx].permute(1, 2, 0).numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1)\n",
    "    ax.imshow(img)\n",
    "    pred_name = class_names[sample_preds[idx]]\n",
    "    true_name = class_names[sample_labels[idx]]\n",
    "    color = 'green' if sample_preds[idx] == sample_labels[idx] else 'red'\n",
    "    ax.set_title(f'Pred: {pred_name}\\nTrue: {true_name}', fontsize=9, color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "ax_cm = fig.add_subplot(gs[2, :])\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax_cm, cbar_kws={'shrink': 0.8})\n",
    "ax_cm.set_xlabel('Predicted')\n",
    "ax_cm.set_ylabel('True')\n",
    "ax_cm.set_title('Confusion Matrix')\n",
    "\n",
    "plt.savefig(PROJECT_ROOT / \"logs\" / \"classification_test_results.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "accuracy = (all_preds == all_labels).mean() * 100\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Total samples: {len(all_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Object Detection (Oxford-IIIT Pet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import OxfordIIITPet\n",
    "\n",
    "class PetDetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split='trainval', transform=None):\n",
    "        self.dataset_seg = OxfordIIITPet(root=root, split=split, target_types='segmentation', download=True)\n",
    "        self.dataset_cat = OxfordIIITPet(root=root, split=split, target_types='category', download=False)\n",
    "        self.transform = transform if transform else transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        self.num_classes = 37\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_seg)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.dataset_seg[idx]\n",
    "        _, class_idx = self.dataset_cat[idx]\n",
    "        \n",
    "        mask_np = np.array(mask)\n",
    "        non_zero = np.where(mask_np > 0)\n",
    "        if len(non_zero[0]) > 0:\n",
    "            y_min, y_max = non_zero[0].min(), non_zero[0].max()\n",
    "            x_min, x_max = non_zero[1].min(), non_zero[1].max()\n",
    "        else:\n",
    "            h, w = mask_np.shape\n",
    "            x_min, y_min, x_max, y_max = 0, 0, w, h\n",
    "        \n",
    "        orig_w, orig_h = img.size\n",
    "        bbox = torch.tensor([\n",
    "            x_min / orig_w * 224,\n",
    "            y_min / orig_h * 224,\n",
    "            x_max / orig_w * 224,\n",
    "            y_max / orig_h * 224\n",
    "        ], dtype=torch.float32)\n",
    "        \n",
    "        img_tensor = self.transform(img)\n",
    "        return img_tensor, bbox, class_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pet Detection Model\n",
    "class PetDetector(pl.LightningModule):\n",
    "    def __init__(self, encoder, num_classes=37, freeze_encoder=True, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['encoder'])\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(768, 512), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256), nn.ReLU(), nn.Linear(256, 4)\n",
    "        )\n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.Linear(768, 512), nn.ReLU(), nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        cls_token = features[:, 0]\n",
    "        bbox = self.bbox_head(cls_token)\n",
    "        cls = self.class_head(cls_token)\n",
    "        return bbox, cls\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, gt_boxes, gt_labels = batch\n",
    "        pred_boxes, pred_cls = self(x)\n",
    "        bbox_loss = F.mse_loss(pred_boxes, gt_boxes)\n",
    "        cls_loss = F.cross_entropy(pred_cls, gt_labels)\n",
    "        loss = bbox_loss + cls_loss\n",
    "        self.log('train_det/bbox_loss', bbox_loss, prog_bar=True)\n",
    "        self.log('train_det/cls_loss', cls_loss, prog_bar=True)\n",
    "        self.log('train_det/total_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, gt_boxes, gt_labels = batch\n",
    "        pred_boxes, pred_cls = self(x)\n",
    "        bbox_loss = F.mse_loss(pred_boxes, gt_boxes)\n",
    "        cls_loss = F.cross_entropy(pred_cls, gt_labels)\n",
    "        loss = bbox_loss + cls_loss\n",
    "        preds = torch.argmax(pred_cls, dim=1)\n",
    "        acc = (preds == gt_labels).float().mean()\n",
    "        self.log('val_det/bbox_loss', bbox_loss)\n",
    "        self.log('val_det/cls_loss', cls_loss)\n",
    "        self.log('val_det/total_loss', loss, prog_bar=True)\n",
    "        self.log('val_det/acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(list(self.bbox_head.parameters()) + list(self.class_head.parameters()), \n",
    "                                lr=self.lr, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection 학습\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "pet_det_dataset = PetDetectionDataset(root=str(PROJECT_ROOT / \"data\"), split='trainval')\n",
    "train_size = int(0.8 * len(pet_det_dataset))\n",
    "val_size = len(pet_det_dataset) - train_size\n",
    "pet_det_train, pet_det_val = random_split(pet_det_dataset, [train_size, val_size])\n",
    "\n",
    "pet_det_train_loader = DataLoader(pet_det_train, batch_size=16, shuffle=True, num_workers=0)\n",
    "pet_det_val_loader = DataLoader(pet_det_val, batch_size=16, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train samples: {train_size}, Val samples: {val_size}\")\n",
    "\n",
    "pet_detector = PetDetector(encoder=encoder, num_classes=37, freeze_encoder=True, lr=1e-3)\n",
    "\n",
    "pet_det_trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=TensorBoardLogger(LOG_DIR, name=\"pet_detection\"),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(dirpath=LOG_DIR / \"checkpoints_pet_det\", \n",
    "                       filename=\"pet-detector-{epoch:02d}-{val_det/total_loss:.4f}\",\n",
    "                       monitor=\"val_det/total_loss\", mode=\"min\", save_top_k=3),\n",
    "        EarlyStopping(monitor=\"val_det/total_loss\", patience=10, mode=\"min\", verbose=True),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ],\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Pet Detection training...\")\n",
    "pet_det_trainer.fit(pet_detector, pet_det_train_loader, pet_det_val_loader)\n",
    "print(\"Pet Detection training completed!\")\n",
    "\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "pet_det_trainer.validate(pet_detector, pet_det_val_loader)\n",
    "print(\"Pet Detection evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection 테스트 시각화\n",
    "pet_detector = pet_detector.to(device)\n",
    "pet_detector.eval()\n",
    "\n",
    "test_loader = DataLoader(pet_det_val, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, gt_boxes, gt_labels in test_loader:\n",
    "        x = x.to(device)\n",
    "        pred_boxes, pred_cls = pet_detector(x)\n",
    "        \n",
    "        x = x.cpu()\n",
    "        pred_boxes = pred_boxes.cpu()\n",
    "        pred_cls = torch.argmax(pred_cls, dim=1).cpu()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "        fig.suptitle('Pet Detection Test Results', fontsize=16, fontweight='bold')\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx in range(8):\n",
    "            ax = axes[idx]\n",
    "            img = x[idx].numpy()\n",
    "            img = img * np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1) + np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
    "            img = np.clip(img, 0, 1).transpose(1, 2, 0)\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            gt_box = gt_boxes[idx].numpy()\n",
    "            pred_box = pred_boxes[idx].numpy()\n",
    "            \n",
    "            rect_gt = patches.Rectangle((gt_box[0], gt_box[1]), gt_box[2]-gt_box[0], gt_box[3]-gt_box[1],\n",
    "                                        linewidth=2, edgecolor='green', facecolor='none', label='GT')\n",
    "            rect_pred = patches.Rectangle((pred_box[0], pred_box[1]), pred_box[2]-pred_box[0], pred_box[3]-pred_box[1],\n",
    "                                          linewidth=2, edgecolor='red', facecolor='none', linestyle='--', label='Pred')\n",
    "            ax.add_patch(rect_gt)\n",
    "            ax.add_patch(rect_pred)\n",
    "            \n",
    "            gt_class = gt_labels[idx].item()\n",
    "            pred_class = pred_cls[idx].item()\n",
    "            color = 'green' if gt_labels[idx] == pred_cls[idx] else 'red'\n",
    "            ax.set_title(f'GT: class {gt_class}\\nPred: class {pred_class}', fontsize=10, color=color)\n",
    "            ax.axis('off')\n",
    "            if idx == 0:\n",
    "                ax.legend(loc='upper right', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PROJECT_ROOT / \"logs\" / \"pet_detection_test_results.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Semantic Segmentation (Oxford-IIIT Pet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pet Segmentation Dataset\n",
    "class PetSegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split='trainval'):\n",
    "        self.dataset = OxfordIIITPet(root=root, split=split, target_types='segmentation', download=True)\n",
    "        self.img_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "        self.mask_transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.NEAREST),\n",
    "            transforms.PILToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, mask = self.dataset[idx]\n",
    "        \n",
    "        img_tensor = self.img_transform(img)\n",
    "        mask_tensor = self.mask_transform(mask).squeeze(0).long()\n",
    "        \n",
    "        mask_tensor[mask_tensor == 2] = 0\n",
    "        mask_tensor[mask_tensor > 0] = 1\n",
    "        \n",
    "        return img_tensor, mask_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pet Segmentation Model\n",
    "class PetSegmenter(pl.LightningModule):\n",
    "    def __init__(self, encoder, num_classes=2, freeze_encoder=True, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['encoder'])\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(768, 384, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(384), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(384, 192, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(192), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(192, 96, kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(96), nn.ReLU(),\n",
    "            nn.ConvTranspose2d(96, num_classes, kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        patch_features = features[:, 1:]\n",
    "        B, N, C = patch_features.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "        patch_features = patch_features.transpose(1, 2).reshape(B, C, H, W)\n",
    "        seg_map = self.seg_head(patch_features)\n",
    "        return seg_map\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, mask = batch\n",
    "        seg_output = self(x)\n",
    "        loss = F.cross_entropy(seg_output, mask)\n",
    "        pred_mask = seg_output.argmax(dim=1)\n",
    "        acc = (pred_mask == mask).float().mean()\n",
    "        self.log('train_seg/loss', loss, prog_bar=True)\n",
    "        self.log('train_seg/acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, mask = batch\n",
    "        seg_output = self(x)\n",
    "        loss = F.cross_entropy(seg_output, mask)\n",
    "        pred_mask = seg_output.argmax(dim=1)\n",
    "        acc = (pred_mask == mask).float().mean()\n",
    "        self.log('val_seg/loss', loss, prog_bar=True)\n",
    "        self.log('val_seg/acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.seg_head.parameters(), lr=self.lr, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation 학습\n",
    "pet_seg_dataset = PetSegmentationDataset(root=str(PROJECT_ROOT / \"data\"), split='trainval')\n",
    "train_size = int(0.8 * len(pet_seg_dataset))\n",
    "val_size = len(pet_seg_dataset) - train_size\n",
    "pet_seg_train, pet_seg_val = random_split(pet_seg_dataset, [train_size, val_size])\n",
    "\n",
    "pet_seg_train_loader = DataLoader(pet_seg_train, batch_size=8, shuffle=True, num_workers=0)\n",
    "pet_seg_val_loader = DataLoader(pet_seg_val, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train samples: {train_size}, Val samples: {val_size}\")\n",
    "\n",
    "pet_segmenter = PetSegmenter(encoder=encoder, num_classes=2, freeze_encoder=True, lr=1e-3)\n",
    "\n",
    "pet_seg_trainer = pl.Trainer(\n",
    "    max_epochs=50,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=TensorBoardLogger(LOG_DIR, name=\"pet_segmentation\"),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(dirpath=LOG_DIR / \"checkpoints_pet_seg\", \n",
    "                       filename=\"pet-segmenter-{epoch:02d}-{val_seg/loss:.4f}\",\n",
    "                       monitor=\"val_seg/loss\", mode=\"min\", save_top_k=3),\n",
    "        EarlyStopping(monitor=\"val_seg/loss\", patience=10, mode=\"min\", verbose=True),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ],\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\nStarting Pet Segmentation training...\")\n",
    "pet_seg_trainer.fit(pet_segmenter, pet_seg_train_loader, pet_seg_val_loader)\n",
    "print(\"Pet Segmentation training completed!\")\n",
    "\n",
    "print(\"\\nEvaluating on validation set...\")\n",
    "pet_seg_trainer.validate(pet_segmenter, pet_seg_val_loader)\n",
    "print(\"Pet Segmentation evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentation 테스트 시각화\n",
    "pet_segmenter = pet_segmenter.to(device)\n",
    "pet_segmenter.eval()\n",
    "\n",
    "test_loader = DataLoader(pet_seg_val, batch_size=8, shuffle=True, num_workers=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, gt_mask in test_loader:\n",
    "        x = x.to(device)\n",
    "        pred_seg = pet_segmenter(x)\n",
    "        \n",
    "        x = x.cpu()\n",
    "        pred_mask = torch.argmax(pred_seg, dim=1).cpu()\n",
    "        gt_mask = gt_mask.cpu()\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 8, figsize=(24, 9))\n",
    "        fig.suptitle('Pet Segmentation Test Results', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for idx in range(8):\n",
    "            img = x[idx].numpy()\n",
    "            img = img * np.array([0.229, 0.224, 0.225]).reshape(3, 1, 1) + np.array([0.485, 0.456, 0.406]).reshape(3, 1, 1)\n",
    "            img = np.clip(img, 0, 1).transpose(1, 2, 0)\n",
    "            \n",
    "            axes[0, idx].imshow(img)\n",
    "            axes[0, idx].set_title('Input', fontsize=10)\n",
    "            axes[0, idx].axis('off')\n",
    "            \n",
    "            axes[1, idx].imshow(gt_mask[idx], cmap='gray', vmin=0, vmax=1)\n",
    "            axes[1, idx].set_title('GT Mask', fontsize=10)\n",
    "            axes[1, idx].axis('off')\n",
    "            \n",
    "            axes[2, idx].imshow(pred_mask[idx], cmap='gray', vmin=0, vmax=1)\n",
    "            acc = ((pred_mask[idx] == gt_mask[idx]).float().mean() * 100).item()\n",
    "            axes[2, idx].set_title(f'Pred (Acc: {acc:.1f}%)', fontsize=10)\n",
    "            axes[2, idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PROJECT_ROOT / \"logs\" / \"pet_segmentation_test_results.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "mean_iou = ((pred_mask == gt_mask).float().mean() * 100).item()\n",
    "print(f\"Mean Pixel Accuracy: {mean_iou:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "## Three Downstream Tasks\n",
    "\n",
    "### 1. Image Classification (CIFAR-10)\n",
    "- Dataset: 50,000 train (45,000 train / 5,000 val), 10,000 test\n",
    "- Model: CLS token → Linear(768→10)\n",
    "- Epochs: 200 (Early Stop: patience=20)\n",
    "- Log: logs/mae_classification/\n",
    "\n",
    "### 2. Object Detection (Oxford-IIIT Pet)\n",
    "- Dataset: ~7,349 images (auto-download, ~800MB)\n",
    "- Classes: 37 classes\n",
    "- Model: CLS token → {Bbox(4), Class(37)}\n",
    "- Epochs: 50 (Early Stop: patience=10)\n",
    "- Log: logs/pet_detection/\n",
    "\n",
    "### 3. Semantic Segmentation (Oxford-IIIT Pet)\n",
    "- Dataset: ~7,349 images with segmentation masks\n",
    "- Classes: 2 classes (background, foreground)\n",
    "- Model: Spatial(768×14×14) → Upsample(4×) → Mask(2×224×224)\n",
    "- Epochs: 50 (Early Stop: patience=10)\n",
    "- Log: logs/pet_segmentation/\n",
    "\n",
    "## TensorBoard\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=logs\n",
    "```\n",
    "\n",
    "Browser: http://localhost:6006\n",
    "\n",
    "Available tasks:\n",
    "- mae_classification\n",
    "- pet_detection\n",
    "- pet_segmentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import TensorBoardLogger, CSVLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "\n",
    "PROJECT_ROOT = Path.cwd() if (Path.cwd() / \"checkpoints\").exists() else Path.cwd().parent\n",
    "WEIGHT_PATH = PROJECT_ROOT / \"checkpoints\" / \"mae_pretrain_vit_base.pth\"\n",
    "LOG_DIR = PROJECT_ROOT / \"logs\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=True)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        return x\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.fc1(x)))\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp = MLP(dim, int(dim * mlp_ratio))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class MAEEncoder(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3,\n",
    "                 embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        class PatchEmbed(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "            def forward(self, x):\n",
    "                return self.proj(x)\n",
    "        \n",
    "        self.patch_embed = PatchEmbed()\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.blocks = nn.ModuleList([Block(embed_dim, num_heads, mlp_ratio) for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.patch_embed(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.pos_embed\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "def load_mae_encoder(weight_path):\n",
    "    model = MAEEncoder()\n",
    "    ckpt = torch.load(weight_path, map_location=\"cpu\", weights_only=False)\n",
    "    if 'model' in ckpt:\n",
    "        state_dict = ckpt['model']\n",
    "    elif 'state_dict' in ckpt:\n",
    "        state_dict = ckpt['state_dict']\n",
    "    else:\n",
    "        state_dict = ckpt\n",
    "    \n",
    "    for prefix in [\"module.\", \"model.\", \"net.\"]:\n",
    "        state_dict = {k[len(prefix):] if k.startswith(prefix) else k: v for k, v in state_dict.items()}\n",
    "    \n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class CIFAR10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir, batch_size=32, num_workers=0):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        datasets.CIFAR10(self.data_dir, train=True, download=True)\n",
    "        datasets.CIFAR10(self.data_dir, train=False, download=True)\n",
    "    \n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            full_train = datasets.CIFAR10(self.data_dir, train=True, transform=self.transform)\n",
    "            train_size = int(0.9 * len(full_train))\n",
    "            val_size = len(full_train) - train_size\n",
    "            self.train_dataset, self.val_dataset = random_split(full_train, [train_size, val_size])\n",
    "        \n",
    "        if stage == 'test' or stage is None:\n",
    "            self.test_dataset = datasets.CIFAR10(self.data_dir, train=False, transform=self.transform)\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, \n",
    "                         shuffle=True, num_workers=self.num_workers)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, \n",
    "                         shuffle=False, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size, \n",
    "                         shuffle=False, num_workers=self.num_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MAEClassifier(pl.LightningModule):\n",
    "    def __init__(self, encoder, num_classes=10, freeze_encoder=True, lr=1e-3, weight_decay=1e-4):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['encoder'])\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.head = nn.Linear(768, num_classes)\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        \n",
    "        self.train_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.val_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        self.test_acc = Accuracy(task='multiclass', num_classes=num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        cls_token = features[:, 0]\n",
    "        return self.head(cls_token)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.train_acc(preds, y)\n",
    "        \n",
    "        self.log('train/loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        self.log('train/acc', acc, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.val_acc(preds, y)\n",
    "        \n",
    "        self.log('val/loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val/acc', acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = self.test_acc(preds, y)\n",
    "        \n",
    "        self.log('test/loss', loss)\n",
    "        self.log('test/acc', acc)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.head.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'monitor': 'val/loss'\n",
    "            }\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = load_mae_encoder(WEIGHT_PATH)\n",
    "print(\"MAE Encoder loaded\")\n",
    "\n",
    "data_module = CIFAR10DataModule(\n",
    "    data_dir=str(PROJECT_ROOT / \"data\"),\n",
    "    batch_size=32,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "model = MAEClassifier(\n",
    "    encoder=encoder,\n",
    "    num_classes=10,\n",
    "    freeze_encoder=True,\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-4\n",
    ")\n",
    "\n",
    "tensorboard_logger = TensorBoardLogger(LOG_DIR, name=\"mae_classification\")\n",
    "csv_logger = CSVLogger(LOG_DIR, name=\"mae_classification\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=LOG_DIR / \"checkpoints\",\n",
    "    filename=\"mae-{epoch:02d}-{val/acc:.4f}\",\n",
    "    monitor=\"val/acc\",\n",
    "    mode=\"max\",\n",
    "    save_top_k=3\n",
    ")\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val/loss\",\n",
    "    patience=20,\n",
    "    mode=\"min\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "lr_monitor = LearningRateMonitor(logging_interval='epoch')\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=[tensorboard_logger, csv_logger],\n",
    "    callbacks=[checkpoint_callback, early_stop_callback, lr_monitor],\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.fit(model, data_module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTesting...\")\n",
    "trainer.test(model, data_module)\n",
    "\n",
    "print(f\"\\nLogs saved to: {LOG_DIR}\")\n",
    "print(f\"TensorBoard: tensorboard --logdir={LOG_DIR}\")\n",
    "print(f\"CSV logs: {LOG_DIR / 'mae_classification' / 'version_0' / 'metrics.csv'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics_file = list((LOG_DIR / 'mae_classification').rglob('metrics.csv'))[-1]\n",
    "df = pd.read_csv(metrics_file)\n",
    "\n",
    "print(\"Available columns:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "train_loss_col = 'train_loss_epoch' if 'train_loss_epoch' in df.columns else 'train/loss_epoch'\n",
    "val_loss_col = 'val_loss' if 'val_loss' in df.columns else 'val/loss'\n",
    "train_acc_col = 'train_acc_epoch' if 'train_acc_epoch' in df.columns else 'train/acc_epoch'\n",
    "val_acc_col = 'val_acc' if 'val_acc' in df.columns else 'val/acc'\n",
    "\n",
    "train_loss = df[['epoch', train_loss_col]].dropna()\n",
    "val_loss = df[['epoch', val_loss_col]].dropna()\n",
    "axes[0].plot(train_loss['epoch'], train_loss[train_loss_col], label='Train Loss', marker='o')\n",
    "axes[0].plot(val_loss['epoch'], val_loss[val_loss_col], label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "train_acc = df[['epoch', train_acc_col]].dropna()\n",
    "val_acc = df[['epoch', val_acc_col]].dropna()\n",
    "axes[1].plot(train_acc['epoch'], train_acc[train_acc_col] * 100, label='Train Acc', marker='o')\n",
    "axes[1].plot(val_acc['epoch'], val_acc[val_acc_col] * 100, label='Val Acc', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy (%)')\n",
    "axes[1].set_title('Training & Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(PROJECT_ROOT / \"logs\" / \"training_curves.png\", dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest Val Acc: {val_acc[val_acc_col].max() * 100:.2f}%\")\n",
    "print(f\"Final Train Acc: {train_acc[train_acc_col].iloc[-1] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Test 결과 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "test_loader = data_module.test_dataloader()\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "sample_images = []\n",
    "sample_preds = []\n",
    "sample_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (x, y) in enumerate(test_loader):\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "        \n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(y.numpy())\n",
    "        \n",
    "        if i == 0:\n",
    "            sample_images = x[:16].cpu()\n",
    "            sample_preds = preds[:16]\n",
    "            sample_labels = y[:16].numpy()\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "fig.suptitle('Classification Test Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "gs = fig.add_gridspec(3, 6, hspace=0.4, wspace=0.3)\n",
    "\n",
    "for idx in range(12):\n",
    "    ax = fig.add_subplot(gs[idx // 6, idx % 6])\n",
    "    img = sample_images[idx].permute(1, 2, 0).numpy()\n",
    "    img = (img * 0.5 + 0.5).clip(0, 1)\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    pred_name = class_names[sample_preds[idx]]\n",
    "    true_name = class_names[sample_labels[idx]]\n",
    "    color = 'green' if sample_preds[idx] == sample_labels[idx] else 'red'\n",
    "    ax.set_title(f'Pred: {pred_name}\\nTrue: {true_name}', fontsize=9, color=color)\n",
    "    ax.axis('off')\n",
    "\n",
    "ax_cm = fig.add_subplot(gs[2, :])\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax_cm, cbar_kws={'shrink': 0.8})\n",
    "ax_cm.set_xlabel('Predicted')\n",
    "ax_cm.set_ylabel('True')\n",
    "ax_cm.set_title('Confusion Matrix')\n",
    "\n",
    "plt.savefig(PROJECT_ROOT / \"logs\" / \"classification_test_results.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "accuracy = (all_preds == all_labels).mean() * 100\n",
    "print(f\"\\nTest Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"Total samples: {len(all_labels)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorBoard 사용법\n",
    "\n",
    "터미널에서 다음 명령어 실행:\n",
    "```bash\n",
    "tensorboard --logdir=logs\n",
    "```\n",
    "\n",
    "그 다음 브라우저에서 http://localhost:6006 접속\n",
    "\n",
    "---\n",
    "\n",
    "## Lightning 로그 구조:\n",
    "```\n",
    "logs/\n",
    "├── mae_classification/\n",
    "│   └── version_0/\n",
    "│       ├── hparams.yaml       # 하이퍼파라미터\n",
    "│       ├── metrics.csv        # CSV 로그\n",
    "│       └── events.out.tfevents.xxx  # TensorBoard 로그\n",
    "└── checkpoints/\n",
    "    └── mae-epoch=XX-val_acc=0.XXXX.ckpt  # 체크포인트\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 로그 내용:\n",
    "- `train/loss`: 배치별 + 에폭별 학습 loss\n",
    "- `train/acc`: 배치별 + 에폭별 학습 정확도\n",
    "- `val/loss`: 에폭별 검증 loss\n",
    "- `val/acc`: 에폭별 검증 정확도\n",
    "- `lr-Adam`: Learning rate 변화\n",
    "- `test/loss`, `test/acc`: 테스트 결과\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 요약\n",
    "\n",
    "## 구현된 3가지 Downstream Tasks (실제 데이터셋):\n",
    "\n",
    "### 1. Image Classification (CIFAR-10)\n",
    "- **Dataset**: 50,000 train (45,000 train / 5,000 val), 10,000 test\n",
    "- **Model**: CLS token → Linear(768→10)\n",
    "- **Epochs**: 200 (Early Stop: patience=20)\n",
    "- **Metrics**: `train/loss`, `train/acc`, `val/loss`, `val/acc`\n",
    "- **Log**: `logs/mae_classification/`\n",
    "- **Result**: ~79% test accuracy\n",
    "\n",
    "### 2. Object Detection (Pascal VOC 2012)\n",
    "- **Dataset**: ~5,717 train, ~5,823 val (실제 이미지)\n",
    "- **Classes**: 20 classes (aeroplane, bicycle, bird, ...)\n",
    "- **Model**: CLS token → {Bbox(4), Class(20)}\n",
    "- **Epochs**: 50 (Early Stop: patience=10)\n",
    "- **Metrics**: `train_det/bbox_loss`, `train_det/cls_loss`, `val_det/acc`\n",
    "- **Log**: `logs/voc_detection/`\n",
    "- **Note**: 가장 큰 객체 하나만 detection (simplified)\n",
    "\n",
    "### 3. Semantic Segmentation (Pascal VOC 2012)\n",
    "- **Dataset**: ~1,464 train, ~1,449 val (실제 이미지 + 마스크)\n",
    "- **Classes**: 21 classes (background + 20 objects)\n",
    "- **Model**: Spatial(768×14×14) → Upsample(4×) → Mask(21×224×224)\n",
    "- **Epochs**: 50 (Early Stop: patience=10)\n",
    "- **Metrics**: `train_seg/loss`, `train_seg/acc`, `val_seg/loss`\n",
    "- **Log**: `logs/voc_segmentation/`\n",
    "\n",
    "---\n",
    "\n",
    "## TensorBoard로 모든 Task 동시에 보기:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=logs\n",
    "```\n",
    "\n",
    "브라우저: http://localhost:6006\n",
    "\n",
    "왼쪽 메뉴에서 각 task별로 전환 가능:\n",
    "- `mae_classification`\n",
    "- `voc_detection`\n",
    "- `voc_segmentation`\n",
    "\n",
    "---\n",
    "\n",
    "## 저장된 파일:\n",
    "\n",
    "- `logs/classification_test_results.png` - Classification confusion matrix\n",
    "- `logs/voc_detection_test_results.png` - Detection bbox visualization  \n",
    "- `logs/voc_segmentation_test_results.png` - Segmentation mask visualization\n",
    "- `logs/checkpoints/` - Classification best models\n",
    "- `logs/checkpoints_voc_det/` - Detection best models\n",
    "- `logs/checkpoints_voc_seg/` - Segmentation best models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_detection_sample(size=224, num_boxes=3):\n",
    "    img = torch.rand(3, size, size)\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    for _ in range(num_boxes):\n",
    "        x1, y1 = np.random.randint(0, size-50, 2)\n",
    "        w, h = np.random.randint(30, 70, 2)\n",
    "        boxes.append([x1, y1, x1+w, y1+h])\n",
    "        labels.append(np.random.randint(0, 10))\n",
    "        img[:, y1:y1+h, x1:x1+w] = torch.rand(3, 1, 1) * 0.5 + 0.5\n",
    "    return img, torch.tensor(boxes, dtype=torch.float32), torch.tensor(labels)\n",
    "\n",
    "class DetectionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, boxes, labels = create_detection_sample()\n",
    "        return img, (boxes, labels)\n",
    "\n",
    "class MAEDetector(pl.LightningModule):\n",
    "    def __init__(self, encoder, num_classes=10, freeze_encoder=True, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['encoder'])\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.bbox_head = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 4)\n",
    "        )\n",
    "        self.class_head = nn.Sequential(\n",
    "            nn.Linear(768, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        self.lr = lr\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        patch_features = features[:, 1:]\n",
    "        pooled = patch_features.mean(dim=1)\n",
    "        bbox = self.bbox_head(pooled)\n",
    "        cls = self.class_head(pooled)\n",
    "        return bbox, cls\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, (gt_boxes, gt_labels) = batch\n",
    "        pred_boxes, pred_cls = self(x)\n",
    "        \n",
    "        bbox_loss = F.mse_loss(pred_boxes, gt_boxes[:, 0])\n",
    "        cls_loss = F.cross_entropy(pred_cls, gt_labels[:, 0])\n",
    "        loss = bbox_loss + cls_loss\n",
    "        \n",
    "        self.log('train/bbox_loss', bbox_loss, prog_bar=True)\n",
    "        self.log('train/cls_loss', cls_loss, prog_bar=True)\n",
    "        self.log('train/total_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, (gt_boxes, gt_labels) = batch\n",
    "        pred_boxes, pred_cls = self(x)\n",
    "        \n",
    "        bbox_loss = F.mse_loss(pred_boxes, gt_boxes[:, 0])\n",
    "        cls_loss = F.cross_entropy(pred_cls, gt_labels[:, 0])\n",
    "        loss = bbox_loss + cls_loss\n",
    "        \n",
    "        self.log('val/bbox_loss', bbox_loss)\n",
    "        self.log('val/cls_loss', cls_loss)\n",
    "        self.log('val/total_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(list(self.bbox_head.parameters()) + list(self.class_head.parameters()), \n",
    "                                lr=self.lr)\n",
    "\n",
    "print(\"Object Detection module defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segmentation_sample(size=224, num_classes=5):\n",
    "    img = torch.rand(3, size, size)\n",
    "    mask = torch.zeros(size, size, dtype=torch.long)\n",
    "    for c in range(1, num_classes):\n",
    "        x, y = np.random.randint(0, size-60, 2)\n",
    "        w, h = np.random.randint(40, 80, 2)\n",
    "        mask[y:y+h, x:x+w] = c\n",
    "        img[:, y:y+h, x:x+w] = torch.tensor([c/num_classes, 0.5, 1-c/num_classes]).view(3, 1, 1)\n",
    "    return img, mask\n",
    "\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return create_segmentation_sample()\n",
    "\n",
    "class MAESegmenter(pl.LightningModule):\n",
    "    def __init__(self, encoder, num_classes=5, freeze_encoder=True, lr=1e-3):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=['encoder'])\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        self.seg_head = nn.Sequential(\n",
    "            nn.ConvTranspose2d(768, 256, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, num_classes, kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.lr = lr\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        patch_features = features[:, 1:]\n",
    "        B, N, C = patch_features.shape\n",
    "        H = W = int(N ** 0.5)\n",
    "        patch_features = patch_features.transpose(1, 2).reshape(B, C, H, W)\n",
    "        seg_map = self.seg_head(patch_features)\n",
    "        return seg_map\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, mask = batch\n",
    "        seg_output = self(x)\n",
    "        loss = F.cross_entropy(seg_output, mask)\n",
    "        \n",
    "        pred_mask = seg_output.argmax(dim=1)\n",
    "        acc = (pred_mask == mask).float().mean()\n",
    "        \n",
    "        self.log('train/seg_loss', loss, prog_bar=True)\n",
    "        self.log('train/seg_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, mask = batch\n",
    "        seg_output = self(x)\n",
    "        loss = F.cross_entropy(seg_output, mask)\n",
    "        \n",
    "        pred_mask = seg_output.argmax(dim=1)\n",
    "        acc = (pred_mask == mask).float().mean()\n",
    "        \n",
    "        self.log('val/seg_loss', loss, prog_bar=True)\n",
    "        self.log('val/seg_acc', acc, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.seg_head.parameters(), lr=self.lr)\n",
    "\n",
    "print(\"Semantic Segmentation module defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Object Detection Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "det_train_dataset = DetectionDataset(num_samples=800)\n",
    "det_val_dataset = DetectionDataset(num_samples=200)\n",
    "\n",
    "det_train_loader = DataLoader(det_train_dataset, batch_size=16, shuffle=True)\n",
    "det_val_loader = DataLoader(det_val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "detector = MAEDetector(encoder=encoder, num_classes=10, freeze_encoder=True, lr=1e-3)\n",
    "\n",
    "det_trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=TensorBoardLogger(LOG_DIR, name=\"mae_detection\"),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(dirpath=LOG_DIR / \"checkpoints_det\", \n",
    "                       filename=\"detector-{epoch:02d}-{val/total_loss:.4f}\",\n",
    "                       monitor=\"val/total_loss\", mode=\"min\", save_top_k=3),\n",
    "        EarlyStopping(monitor=\"val/total_loss\", patience=20, mode=\"min\", verbose=True),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ],\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "det_trainer.fit(detector, det_train_loader, det_val_loader)\n",
    "print(\"Detection training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Test 결과 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "test_dataset = DetectionDataset(num_samples=16)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "detector = detector.to(device)\n",
    "detector.eval()\n",
    "with torch.no_grad():\n",
    "    for x, (gt_boxes, gt_labels) in test_loader:\n",
    "        x = x.to(device)\n",
    "        pred_boxes, pred_cls = detector(x)\n",
    "        \n",
    "        x = x.cpu()\n",
    "        pred_boxes = pred_boxes.cpu()\n",
    "        pred_cls = torch.argmax(pred_cls, dim=1).cpu()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "        fig.suptitle('Object Detection Test Results', fontsize=16, fontweight='bold')\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx in range(8):\n",
    "            ax = axes[idx]\n",
    "            img = x[idx].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            gt_box = gt_boxes[idx][0].numpy()\n",
    "            pred_box = pred_boxes[idx].numpy()\n",
    "            \n",
    "            rect_gt = patches.Rectangle((gt_box[0], gt_box[1]), gt_box[2]-gt_box[0], gt_box[3]-gt_box[1],\n",
    "                                        linewidth=2, edgecolor='green', facecolor='none', label='GT')\n",
    "            rect_pred = patches.Rectangle((pred_box[0], pred_box[1]), pred_box[2]-pred_box[0], pred_box[3]-pred_box[1],\n",
    "                                          linewidth=2, edgecolor='red', facecolor='none', linestyle='--', label='Pred')\n",
    "            \n",
    "            ax.add_patch(rect_gt)\n",
    "            ax.add_patch(rect_pred)\n",
    "            \n",
    "            gt_label = gt_labels[idx][0].item()\n",
    "            pred_label = pred_cls[idx].item()\n",
    "            ax.set_title(f'GT: class {gt_label} | Pred: class {pred_label}', fontsize=10)\n",
    "            ax.axis('off')\n",
    "            \n",
    "            if idx == 0:\n",
    "                ax.legend(loc='upper right', fontsize=8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PROJECT_ROOT / \"logs\" / \"detection_test_results.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "print(\"Detection visualization completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Semantic Segmentation Training\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "seg_train_dataset = SegmentationDataset(num_samples=800)\n",
    "seg_val_dataset = SegmentationDataset(num_samples=200)\n",
    "\n",
    "seg_train_loader = DataLoader(seg_train_dataset, batch_size=16, shuffle=True)\n",
    "seg_val_loader = DataLoader(seg_val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "segmenter = MAESegmenter(encoder=encoder, num_classes=5, freeze_encoder=True, lr=1e-3)\n",
    "\n",
    "seg_trainer = pl.Trainer(\n",
    "    max_epochs=200,\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    logger=TensorBoardLogger(LOG_DIR, name=\"mae_segmentation\"),\n",
    "    callbacks=[\n",
    "        ModelCheckpoint(dirpath=LOG_DIR / \"checkpoints_seg\", \n",
    "                       filename=\"segmenter-{epoch:02d}-{val/seg_loss:.4f}\",\n",
    "                       monitor=\"val/seg_loss\", mode=\"min\", save_top_k=3),\n",
    "        EarlyStopping(monitor=\"val/seg_loss\", patience=20, mode=\"min\", verbose=True),\n",
    "        LearningRateMonitor(logging_interval='epoch')\n",
    "    ],\n",
    "    log_every_n_steps=10,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "seg_trainer.fit(segmenter, seg_train_loader, seg_val_loader)\n",
    "print(\"Segmentation training completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 전체 요약\n",
    "\n",
    "## 구현된 3가지 Downstream Tasks:\n",
    "\n",
    "### 1. Image Classification (CIFAR-10)\n",
    "- Dataset: 50,000 train (45,000 train / 5,000 val), 10,000 test\n",
    "- Model: CLS token → Linear(768→10)\n",
    "- Epochs: 200 (Early Stop: patience=20)\n",
    "- Metrics: `train/loss`, `train/acc`, `val/loss`, `val/acc`\n",
    "- Log: `logs/mae_classification/`\n",
    "\n",
    "### 2. Object Detection (Synthetic)\n",
    "- Dataset: 800 train, 200 val (synthetic)\n",
    "- Model: Patch tokens → Pool → {Bbox(4), Class(10)}\n",
    "- Epochs: 200 (Early Stop: patience=20)\n",
    "- Metrics: `train/bbox_loss`, `train/cls_loss`, `val/total_loss`\n",
    "- Log: `logs/mae_detection/`\n",
    "\n",
    "### 3. Semantic Segmentation (Synthetic)\n",
    "- Dataset: 800 train, 200 val (synthetic)\n",
    "- Model: Spatial(768×14×14) → Upsample(4×) → Mask(5×224×224)\n",
    "- Epochs: 200 (Early Stop: patience=20)\n",
    "- Metrics: `train/seg_loss`, `train/seg_acc`, `val/seg_loss`\n",
    "- Log: `logs/mae_segmentation/`\n",
    "\n",
    "---\n",
    "\n",
    "## TensorBoard로 모든 Task 동시에 보기:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir=logs\n",
    "```\n",
    "\n",
    "브라우저: http://localhost:6006\n",
    "\n",
    "왼쪽 메뉴에서 각 task별로 전환 가능:\n",
    "- `mae_classification`\n",
    "- `mae_detection`\n",
    "- `mae_segmentation`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Test 결과 시각화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = SegmentationDataset(num_samples=8)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "segmenter = segmenter.to(device)\n",
    "segmenter.eval()\n",
    "with torch.no_grad():\n",
    "    for x, gt_mask in test_loader:\n",
    "        x = x.to(device)\n",
    "        pred_seg = segmenter(x)\n",
    "        \n",
    "        x = x.cpu()\n",
    "        pred_mask = torch.argmax(pred_seg, dim=1).cpu()\n",
    "        gt_mask = gt_mask.cpu()\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 8, figsize=(20, 8))\n",
    "        fig.suptitle('Semantic Segmentation Test Results', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for idx in range(8):\n",
    "            img = x[idx].permute(1, 2, 0).numpy()\n",
    "            \n",
    "            axes[0, idx].imshow(img)\n",
    "            axes[0, idx].set_title('Input', fontsize=10)\n",
    "            axes[0, idx].axis('off')\n",
    "            \n",
    "            axes[1, idx].imshow(gt_mask[idx], cmap='tab10', vmin=0, vmax=9)\n",
    "            axes[1, idx].set_title('GT Mask', fontsize=10)\n",
    "            axes[1, idx].axis('off')\n",
    "            \n",
    "            axes[2, idx].imshow(pred_mask[idx], cmap='tab10', vmin=0, vmax=9)\n",
    "            iou = ((pred_mask[idx] == gt_mask[idx]).float().mean() * 100).item()\n",
    "            axes[2, idx].set_title(f'Pred (IoU: {iou:.1f}%)', fontsize=10)\n",
    "            axes[2, idx].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(PROJECT_ROOT / \"logs\" / \"segmentation_test_results.png\", dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        break\n",
    "\n",
    "mean_iou = ((pred_mask == gt_mask).float().mean() * 100).item()\n",
    "print(f\"Mean IoU: {mean_iou:.2f}%\")\n",
    "print(\"Segmentation visualization completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAE from Scratch",
   "language": "python",
   "name": "mae-from-scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "interpreter": {
   "hash": "F:\\\\mae-from-scratch\\\\.venv\\\\Scripts\\\\python.exe"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}