{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb395e20",
   "metadata": {},
   "source": [
    "**기존 비전 학습의 한계**\n",
    "\n",
    "- Vision Transformer (ViT, 2020)는 대용량 데이터와 사전학습(pretraining)을 통해 뛰어난 성능을 냈지만,\n",
    "자기지도학습(self-supervised learning)으로 효율적인 표현을 학습하기는 어려웠음.\n",
    " \n",
    "- SimCLR, MoCo, BYOL 같은 contrastive learning 기법은 이미지 쌍 간의 관계를 학습하지만\n",
    "    - positive/negative 쌍 설계가 복잡함\n",
    "    - augmentation 의존도가 높음\n",
    "    - 계산량이 많음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c878cb",
   "metadata": {},
   "source": [
    "**배경**\n",
    "\n",
    "- BERT (2018)는 입력의 일부 토큰을 masking → 복원하는 단순한 자기지도(pretext) task로 뛰어난 성능을 보여줌.\n",
    "- 저자들은 “이미지에서도 이런 mask reconstruction 방식을 적용할 수 있지 않을까?” 라는 아이디어에서 출발.\n",
    "\n",
    "핵심 목표\n",
    "\n",
    "이미지를 일부 가리고(masking), 나머지로부터 원본을 복원하도록 학습하면 좋은 시각 표현을 얻을 수 있지 않을까?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d499ff",
   "metadata": {},
   "source": [
    "입력의 대부분을 가리는 Masked Image Modeling (MIM)\n",
    "\n",
    "- 입력 이미지를 ViT patch 단위(예: 16×16)로 나누고 이 중 랜덤하게 75% 정도를 mask.\n",
    "    \n",
    "- 나머지 25%의 patch만 인코더에 입력.\n",
    "- 디코더가 mask된 패치를 픽셀 단위로 복원하도록 학습.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b08e4",
   "metadata": {},
   "source": [
    "**비대칭 구조 (Asymmetric Encoder–Decoder Architecture)**\n",
    "\n",
    "- Encoder:\n",
    "    - 입력의 *일부(25%)*만 처리 → 계산량 절감\n",
    "    - ViT 구조 (Patch Embedding + Transformer Layers)\n",
    "- Decoder:\n",
    "    - mask된 토큰을 포함한 전체 토큰을 입력받음\n",
    "    - 더 얕고 가벼움 (few Transformer blocks)\n",
    "    - 원본 이미지를 복원(reconstruction)\n",
    "\n",
    "**→ Encoder는 representation 학습, Decoder는 복원 전용.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba003f",
   "metadata": {},
   "source": [
    "![mae](F:\\mae-from-scratch\\notebook\\mae.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4933cdf0",
   "metadata": {},
   "source": [
    "**Reconstruction Target**\n",
    "\n",
    "- 입력 이미지를 patch 단위로 normalize된 pixel value로 복원\n",
    "    - 즉, 색상 정보에 집중하지 않고 구조적 패턴에 집중\n",
    "- (이후 연구에서 feature-level reconstruction도 시도됨 — e.g., iBOT, BEiT 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9256cb",
   "metadata": {},
   "source": [
    "**Pretraining → Finetuning**\n",
    "\n",
    "1. Pretrain:\n",
    "    \n",
    "    Masked reconstruction task로 대규모 데이터(예: ImageNet-1K)에서 학습\n",
    "    \n",
    "2. Finetune:\n",
    "    \n",
    "    Encoder만 남기고 classification head를 붙여 downstream task 학습"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MAE from Scratch",
   "language": "python",
   "name": "mae-from-scratch"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
