{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16c84e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Contents\n",
    "# =========================\n",
    "# 1) Setup\n",
    "# 2) module import\n",
    "# 3) path import\n",
    "# 4) util function\n",
    "# 5) Configuration\n",
    "# 6) Train/validation\n",
    "# 7) test\n",
    "# 8) Final pipeline\n",
    "# 9) Conclusion\n",
    "# =========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "124f82be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[env] device=cuda, torch=2.5.1+cu121, timm=1.0.21\n"
     ]
    }
   ],
   "source": [
    "# 1) Setup & Imports\n",
    "import os, random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, utils as vutils\n",
    "\n",
    "import timm  # ViT backbone\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "set_seed(42)\n",
    "\n",
    "print(f\"[env] device={device}, torch={torch.__version__}, timm={timm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e09b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Configuration & Paths\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    mask_ratio: float = 0.75\n",
    "    enc_name: str = \"vit_base_patch16_224\"  # timm 모델명\n",
    "    dec_dim: int = 384\n",
    "    dec_depth: int = 6\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "    lr: float = 1e-4\n",
    "    wd: float = 0.05\n",
    "    epochs: int = 1 \n",
    "    save_dir: str = \"./runs/mae_demo\"\n",
    "    data_root: str = \"./data\"   \n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# 디렉토리 준비\n",
    "os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "os.makedirs(f\"{cfg.save_dir}/ckpt\", exist_ok=True)\n",
    "os.makedirs(f\"{cfg.save_dir}/viz\",  exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37d35d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Utility Functions\n",
    "\n",
    "#이미지를 패치 토큰들로 나눔\n",
    "class Patchify(nn.Module):\n",
    "    def __init__(self, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.p = patch_size\n",
    "    def forward(self, imgs):  # (B,C,H,W) -> (B, N, P*P*C)\n",
    "        B, C, H, W = imgs.shape\n",
    "        p = self.p\n",
    "        assert H % p == 0 and W % p == 0, \"H,W must be divisible by patch_size\"\n",
    "        h, w = H // p, W // p\n",
    "        x = imgs.reshape(B, C, h, p, w, p).permute(0, 2, 4, 3, 5, 1).reshape(B, h*w, p*p*C)\n",
    "        return x\n",
    "#토큰을 다시 이미지로 합침침\n",
    "class Unpatchify(nn.Module):\n",
    "    def __init__(self, img_hw=224, patch_size=16, c=3):\n",
    "        super().__init__()\n",
    "        self.img_hw = img_hw; self.p = patch_size; self.c = c\n",
    "    def forward(self, x):  # (B,N,P*P*C) -> (B,C,H,W)\n",
    "        B, N, D = x.shape\n",
    "        p, c = self.p, self.c\n",
    "        h = w = self.img_hw // p\n",
    "        x = x.reshape(B, h, w, p, p, c).permute(0,5,1,3,2,4).reshape(B, c, h*p, w*p)\n",
    "        return x\n",
    "\n",
    "def random_mask_indices(num_patches, mask_ratio=0.75):\n",
    "    n_mask = int(num_patches * mask_ratio)\n",
    "    ids = torch.randperm(num_patches)\n",
    "    mask_ids = ids[:n_mask]\n",
    "    keep_ids = ids[n_mask:]\n",
    "    return keep_ids, mask_ids\n",
    "\n",
    "def save_grid(tensor, path, nrow=8):\n",
    "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    vutils.save_image(tensor.clamp(0,1), str(path), nrow=nrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e486416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) ViT Token Extractor\n",
    "# timm의 ViT에서 \"토큰 시퀀스 (B, N+1, D)\"를 얻기 위한 도우미\n",
    "# (forward_features는 보통 풀링 벡터를 내므로, 내부 모듈을 통해 토큰을 직접 구성)\n",
    "\n",
    "def vit_tokens_from_timm(vit: nn.Module, imgs: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Return: (B, N+1, D)  [CLS + patch tokens]\n",
    "    Assumes vit has attributes: patch_embed, cls_token, pos_embed, blocks, norm\n",
    "    \"\"\"\n",
    "    x = vit.patch_embed(imgs)  # (B, N, D)\n",
    "    B, N, D = x.shape\n",
    "    cls_token = vit.cls_token.expand(B, -1, -1)  # (B,1,D)\n",
    "    x = torch.cat((cls_token, x), dim=1)         # (B, N+1, D)\n",
    "\n",
    "    # pos embed (224 입력 가정; 사이즈 다르면 보간 필요)\n",
    "    if getattr(vit, \"pos_embed\", None) is not None:\n",
    "        x = x + vit.pos_embed\n",
    "\n",
    "    x = vit.pos_drop(x)\n",
    "    for blk in vit.blocks:\n",
    "        x = blk(x)\n",
    "    x = vit.norm(x)\n",
    "    return x  # (B, N+1, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff56717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) MAE Model\n",
    "\n",
    "class MAE(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # Encoder (timm ViT)\n",
    "        self.encoder = timm.create_model(cfg.enc_name, pretrained=False)\n",
    "        emb_dim = self.encoder.embed_dim\n",
    "\n",
    "        # Decoder (lightweight TransformerEncoder)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, cfg.dec_dim))\n",
    "        total_tokens = (cfg.img_size // cfg.patch_size) ** 2\n",
    "        self.dec_pos = nn.Parameter(torch.zeros(1, total_tokens, cfg.dec_dim))\n",
    "        self.enc_to_dec = nn.Linear(emb_dim, cfg.dec_dim)\n",
    "\n",
    "        nhead = max(1, min(8, cfg.dec_dim // 64))  # 384→6\n",
    "        layer = nn.TransformerEncoderLayer(d_model=cfg.dec_dim, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.TransformerEncoder(layer, num_layers=cfg.dec_depth)\n",
    "\n",
    "        self.head = nn.Linear(cfg.dec_dim, cfg.patch_size * cfg.patch_size * 3)\n",
    "\n",
    "        self.patchify = Patchify(cfg.patch_size)\n",
    "        self.unpatchify = Unpatchify(cfg.img_size, cfg.patch_size, c=3)\n",
    "\n",
    "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.dec_pos,   std=0.02)\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor):\n",
    "        # 1) target patches\n",
    "        target = self.patchify(imgs)        # (B, N, P2*C)\n",
    "        B, N, D = target.shape\n",
    "\n",
    "        # 2) mask indices per-sample\n",
    "        keep_ids, mask_ids = [], []\n",
    "        for _ in range(B):\n",
    "            k, m = random_mask_indices(N, self.cfg.mask_ratio)\n",
    "            keep_ids.append(k); mask_ids.append(m)\n",
    "        keep_ids = torch.stack(keep_ids, 0).to(imgs.device)  # (B, Nk)\n",
    "        mask_ids = torch.stack(mask_ids, 0).to(imgs.device)  # (B, Nm)\n",
    "\n",
    "        # 3) Encoder tokens (CLS 제외)\n",
    "        enc_all = vit_tokens_from_timm(self.encoder, imgs)   # (B, N+1, De)\n",
    "        enc_tokens = enc_all[:, 1:, :]                       # (B, N,   De)\n",
    "\n",
    "        # keep 위치만 취득\n",
    "        enc_kept = torch.gather(\n",
    "            enc_tokens, dim=1,\n",
    "            index=keep_ids.unsqueeze(-1).expand(-1, -1, enc_tokens.size(-1))\n",
    "        )  # (B, Nk, De)\n",
    "\n",
    "        # 4) Decoder input: kept + mask\n",
    "        Nk = enc_kept.size(1); Nm = mask_ids.size(1)\n",
    "        dec_kept = self.enc_to_dec(enc_kept)      # (B, Nk, Dd)\n",
    "        dec_mask = self.mask_token.expand(B, Nm, -1)\n",
    "        dec_in = torch.cat([dec_kept, dec_mask], dim=1) + self.dec_pos[:, :Nk+Nm, :]\n",
    "\n",
    "        dec_out = self.decoder(dec_in)            # (B, Nk+Nm, Dd)\n",
    "        pred = self.head(dec_out[:, Nk:, :])      # (B, Nm, P2*C)\n",
    "\n",
    "        target_masked = torch.gather(\n",
    "            target, dim=1,\n",
    "            index=mask_ids.unsqueeze(-1).expand(-1, -1, target.size(-1))\n",
    "        )\n",
    "        loss = F.mse_loss(pred, target_masked)\n",
    "        return loss, pred, (keep_ids, mask_ids), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19566a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[data] ImageFolder not found — using FakeData for demo.\n"
     ]
    }
   ],
   "source": [
    "# 6) Datasets & Dataloaders\n",
    "\n",
    "def build_dataloaders(cfg: CFG):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize((cfg.img_size, cfg.img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_dir = Path(cfg.data_root) / \"train\"\n",
    "    val_dir   = Path(cfg.data_root) / \"val\"\n",
    "\n",
    "    if train_dir.exists() and val_dir.exists():\n",
    "        print(f\"[data] Using ImageFolder at {cfg.data_root}\")\n",
    "        train_ds = datasets.ImageFolder(str(train_dir), transform=tfm)\n",
    "        val_ds   = datasets.ImageFolder(str(val_dir),   transform=tfm)\n",
    "    else:\n",
    "        print(\"[data] ImageFolder not found — using FakeData for demo.\")\n",
    "        from torchvision.datasets import FakeData\n",
    "        train_ds = FakeData(size=256, image_size=(3, cfg.img_size, cfg.img_size), transform=tfm)\n",
    "        val_ds   = FakeData(size=64,  image_size=(3, cfg.img_size, cfg.img_size), transform=tfm)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "    return train_dl, val_dl\n",
    "\n",
    "train_dl, val_dl = build_dataloaders(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb001eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Train / Validate / Test\n",
    "\n",
    "def train_one_epoch(model, dl, opt, epoch, cfg: CFG):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for imgs, _ in dl:\n",
    "        imgs = imgs.to(device)\n",
    "        loss, pred, idx, target = model(imgs)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item() * imgs.size(0)\n",
    "    return total / len(dl.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dl, cfg: CFG, save_samples=False):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    for i, (imgs, _) in enumerate(dl):\n",
    "        imgs = imgs.to(device)\n",
    "        loss, pred, (keep_ids, mask_ids), target = model(imgs)\n",
    "        total += loss.item() * imgs.size(0)\n",
    "        if save_samples and i == 0:\n",
    "            save_grid(imgs[:16].cpu(), f\"{cfg.save_dir}/viz/input_epoch.jpg\", nrow=4)\n",
    "    return total / len(dl.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_reconstruction(model, dl, cfg: CFG):\n",
    "    model.eval()\n",
    "    te_loss = None\n",
    "    for i, (imgs, _) in enumerate(dl):\n",
    "        imgs = imgs.to(device)\n",
    "        loss, pred, (keep_ids, mask_ids), target = model(imgs)\n",
    "        te_loss = loss.item()\n",
    "        # 데모에선 입력만 저장 (전체 복원 이미지는 ids_restore 로직이 필요)\n",
    "        if i == 0:\n",
    "            save_grid(imgs[:16].cpu(), f\"{cfg.save_dir}/viz/test_input.jpg\", nrow=4)\n",
    "            break\n",
    "    return float(te_loss) if te_loss is not None else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d27e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[001] train 0.3982 | val 0.2139\n",
      "[DONE] best_val=0.2139 | test_recon_loss=0.2138\n",
      "[ARTIFACTS] F:\\mae-from-scratch\\notebook\\runs\\mae_demo\n"
     ]
    }
   ],
   "source": [
    "# 8) Inference & Visualization Preview\n",
    "\n",
    "model = MAE(cfg).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
    "sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
    "\n",
    "best_val = 1e9\n",
    "for epoch in range(1, cfg.epochs+1):\n",
    "    tr = train_one_epoch(model, train_dl, opt, epoch, cfg)\n",
    "    va = validate(model, val_dl, cfg, save_samples=(epoch % 1 == 0))\n",
    "    sch.step()\n",
    "    print(f\"[{epoch:03d}] train {tr:.4f} | val {va:.4f}\")\n",
    "    if va < best_val:\n",
    "        best_val = va\n",
    "        torch.save(model.state_dict(), f\"{cfg.save_dir}/ckpt/mae_best.pth\")\n",
    "\n",
    "te = test_reconstruction(model, val_dl, cfg)\n",
    "print(f\"[DONE] best_val={best_val:.4f} | test_recon_loss={te:.4f}\")\n",
    "print(f\"[ARTIFACTS] {Path(cfg.save_dir).resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mae-from-scratch)",
   "language": "python",
   "name": "mae-from-scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
