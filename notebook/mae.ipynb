{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c84e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Contents\n",
    "# =========================\n",
    "# 1) Setup\n",
    "# 2) module import\n",
    "# 3) path import\n",
    "# 4) util function\n",
    "# 5) Configuration\n",
    "# 6) Train/validation\n",
    "# 7) test\n",
    "# 8) Final pipeline\n",
    "# 9) Conclusion\n",
    "# =========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124f82be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup & Imports\n",
    "import os, random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, utils as vutils\n",
    "\n",
    "import timm  # ViT backbone\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "set_seed(42)\n",
    "\n",
    "print(f\"[env] device={device}, torch={torch.__version__}, timm={timm.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e09b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Configuration & Paths\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    img_size: int = 224\n",
    "    patch_size: int = 16\n",
    "    mask_ratio: float = 0.75\n",
    "    enc_name: str = \"vit_base_patch16_224\"  # timm 모델명\n",
    "    dec_dim: int = 384\n",
    "    dec_depth: int = 6\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 4\n",
    "    lr: float = 1e-4\n",
    "    wd: float = 0.05\n",
    "    epochs: int = 1 \n",
    "    save_dir: str = \"./runs/\"\n",
    "    data_root: str = \"./data\"   \n",
    "\n",
    "cfg = CFG()\n",
    "\n",
    "# 디렉토리 준비\n",
    "os.makedirs(cfg.save_dir, exist_ok=True)\n",
    "os.makedirs(f\"{cfg.save_dir}/ckpt\", exist_ok=True)\n",
    "os.makedirs(f\"{cfg.save_dir}/viz\",  exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d35d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Utility Functions\n",
    "\n",
    "#이미지를 패치 토큰들로 나눔\n",
    "class Patchify(nn.Module):\n",
    "    def __init__(self, patch_size=16):\n",
    "        super().__init__()\n",
    "        self.p = patch_size\n",
    "    def forward(self, imgs):  # (B,C,H,W) -> (B, N, P*P*C)\n",
    "        B, C, H, W = imgs.shape\n",
    "        p = self.p\n",
    "        assert H % p == 0 and W % p == 0, \n",
    "        h, w = H // p, W // p\n",
    "        x = imgs.reshape(B, C, h, p, w, p).permute(0, 2, 4, 3, 5, 1).reshape(B, h*w, p*p*C)\n",
    "        return x\n",
    "#토큰을 다시 이미지로 합침침\n",
    "class Unpatchify(nn.Module):\n",
    "    def __init__(self, img_hw=224, patch_size=16, c=3):\n",
    "        super().__init__()\n",
    "        self.img_hw = img_hw; self.p = patch_size; self.c = c\n",
    "    def forward(self, x):  # (B,N,P*P*C) -> (B,C,H,W)\n",
    "        B, N, D = x.shape\n",
    "        p, c = self.p, self.c\n",
    "        h = w = self.img_hw // p\n",
    "        x = x.reshape(B, h, w, p, p, c).permute(0,5,1,3,2,4).reshape(B, c, h*p, w*p)\n",
    "        return x\n",
    "\n",
    "def random_mask_indices(num_patches, mask_ratio=0.75):\n",
    "    n_mask = int(num_patches * mask_ratio)\n",
    "    ids = torch.randperm(num_patches)\n",
    "    mask_ids = ids[:n_mask] #가릴거\n",
    "    keep_ids = ids[n_mask:] #보여줄거\n",
    "    return keep_ids, mask_ids\n",
    "\n",
    "def save_grid(tensor, path, nrow=8):\n",
    "    path = Path(path); path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    vutils.save_image(tensor.clamp(0,1), str(path), nrow=nrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e486416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) ViT Token Extractor\n",
    "\n",
    "def vit_tokens_from_timm(vit: nn.Module, patches: torch.Tensor, keep_ids: torch.Tensor):\n",
    "    B, N, patch_dim = patches.shape\n",
    "    \n",
    "    # 1) Keep 패치만 선택\n",
    "    patches_keep = torch.gather(\n",
    "        patches, dim=1,\n",
    "        index=keep_ids.unsqueeze(-1).expand(-1, -1, patch_dim)\n",
    "    )  # (B, Nk, P*P*C)\n",
    "    \n",
    "    # 2) Patch embedding\n",
    "    embed_dim = vit.embed_dim\n",
    "    conv_weight = vit.patch_embed.proj.weight  # (D, C, P, P)\n",
    "    linear_weight = conv_weight.view(embed_dim, -1)  # (D, P*P*C)\n",
    "    x = F.linear(patches_keep, linear_weight, vit.patch_embed.proj.bias)  # (B, Nk, D)\n",
    "    \n",
    "    # 3) CLS token 추가\n",
    "    cls_token = vit.cls_token.expand(B, -1, -1)  # (B, 1, D)\n",
    "    x = torch.cat((cls_token, x), dim=1)  # (B, Nk+1, D)\n",
    "    \n",
    "    # 4) Position embedding\n",
    "    if getattr(vit, \"pos_embed\", None) is not None:\n",
    "        cls_pos = vit.pos_embed[:, :1, :]\n",
    "        keep_pos = torch.gather(\n",
    "            vit.pos_embed[:, 1:, :].expand(B, -1, -1), dim=1,\n",
    "            index=keep_ids.unsqueeze(-1).expand(-1, -1, embed_dim)\n",
    "        )\n",
    "        pos_embed = torch.cat([cls_pos.expand(B, -1, -1), keep_pos], dim=1)\n",
    "        x = x + pos_embed\n",
    "    \n",
    "    # 5) Transformer blocks\n",
    "    x = vit.pos_drop(x)\n",
    "    for blk in vit.blocks:\n",
    "        x = blk(x)\n",
    "    x = vit.norm(x)\n",
    "    \n",
    "    return x[:, 1:, :]  # CLS 제외, (B, Nk, D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff56717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) MAE Model\n",
    "\n",
    "class MAE(nn.Module):\n",
    "    def __init__(self, cfg: CFG):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        # Encoder: ViT backbone (timm 사용)\n",
    "        self.encoder = timm.create_model(cfg.enc_name, pretrained=False)\n",
    "        emb_dim = self.encoder.embed_dim  # 768 (ViT-Base)\n",
    "\n",
    "        # Decoder: 가벼운 Transformer\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, cfg.dec_dim))  # 마스크된 패치를 대체할 학습 가능한 토큰\n",
    "        total_tokens = (cfg.img_size // cfg.patch_size) ** 2  # 196 (224/16)^2\n",
    "        self.dec_pos = nn.Parameter(torch.zeros(1, total_tokens, cfg.dec_dim))  # 디코더용 position embedding\n",
    "        self.enc_to_dec = nn.Linear(emb_dim, cfg.dec_dim)  # 인코더(768) -> 디코더(384) 차원 변환\n",
    "\n",
    "        nhead = max(1, min(8, cfg.dec_dim // 64))  # attention head 수 (384->6)\n",
    "        layer = nn.TransformerEncoderLayer(d_model=cfg.dec_dim, nhead=nhead, batch_first=True)\n",
    "        self.decoder = nn.TransformerEncoder(layer, num_layers=cfg.dec_depth)  # 6-layer Transformer\n",
    "\n",
    "        self.head = nn.Linear(cfg.dec_dim, cfg.patch_size * cfg.patch_size * 3)  # 384 -> 768 (16x16x3 픽셀)\n",
    "\n",
    "        self.patchify = Patchify(cfg.patch_size)\n",
    "        self.unpatchify = Unpatchify(cfg.img_size, cfg.patch_size, c=3)\n",
    "\n",
    "        nn.init.trunc_normal_(self.mask_token, std=0.02)\n",
    "        nn.init.trunc_normal_(self.dec_pos,   std=0.02)\n",
    "\n",
    "    def forward(self, imgs: torch.Tensor): \n",
    "        target = self.patchify(imgs)  # (B, N, P*P*C) = (B, 196, 768)\n",
    "        B, N, D = target.shape\n",
    "\n",
    "        keep_ids, mask_ids = [], []  # 각 샘플마다 다르게 마스킹\n",
    "        for _ in range(B):\n",
    "            k, m = random_mask_indices(N, self.cfg.mask_ratio)\n",
    "            keep_ids.append(k); mask_ids.append(m)\n",
    "        keep_ids = torch.stack(keep_ids, 0).to(imgs.device)  # (B, Nk) = (B, 49) keep할 패치 인덱스\n",
    "        mask_ids = torch.stack(mask_ids, 0).to(imgs.device)  # (B, Nm) = (B, 147) 가릴 패치 인덱스\n",
    "\n",
    "        enc_kept = vit_tokens_from_timm(self.encoder, target, keep_ids)  # (B, Nk, De) = (B, 49, 768)\n",
    "        \n",
    "        Nk = enc_kept.size(1) \n",
    "        Nm = mask_ids.size(1) \n",
    "        \n",
    "        dec_kept = self.enc_to_dec(enc_kept)  \n",
    "        dec_mask = self.mask_token.expand(B, Nm, -1) \n",
    "        dec_in = torch.cat([dec_kept, dec_mask], dim=1) + self.dec_pos[:, :Nk+Nm, :]  # (B, 196, 384)\n",
    "\n",
    "        # 5) 디코더로 복원\n",
    "        dec_out = self.decoder(dec_in)  # (B, 196, 384)\n",
    "        pred = self.head(dec_out[:, Nk:, :])  # (B, 147, 768) 마스크된 부분만 예측\n",
    "\n",
    "        # 6) Loss 계산: 마스크된 패치의 정답과 예측 비교\n",
    "        target_masked = torch.gather(\n",
    "            target, dim=1,\n",
    "            index=mask_ids.unsqueeze(-1).expand(-1, -1, target.size(-1))\n",
    "        )  # (B, 147, 768) 마스크된 패치의 정답\n",
    "        loss = F.mse_loss(pred, target_masked)  # 픽셀 단위 MSE\n",
    "        \n",
    "        return loss, pred, (keep_ids, mask_ids), target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19566a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Datasets & Dataloaders\n",
    "\n",
    "def build_dataloaders(cfg: CFG):\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Resize((cfg.img_size, cfg.img_size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    train_dir = Path(cfg.data_root) / \"train\"\n",
    "    val_dir   = Path(cfg.data_root) / \"val\"\n",
    "\n",
    "    if train_dir.exists() and val_dir.exists():\n",
    "        print(f\"[data] Using ImageFolder at {cfg.data_root}\")\n",
    "        train_ds = datasets.ImageFolder(str(train_dir), transform=tfm)\n",
    "        val_ds   = datasets.ImageFolder(str(val_dir),   transform=tfm)\n",
    "    else:\n",
    "        print(\"[data] ImageFolder not found — using FakeData for demo.\")\n",
    "        from torchvision.datasets import FakeData\n",
    "        train_ds = FakeData(size=256, image_size=(3, cfg.img_size, cfg.img_size), transform=tfm)\n",
    "        val_ds   = FakeData(size=64,  image_size=(3, cfg.img_size, cfg.img_size), transform=tfm)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size=cfg.batch_size, shuffle=True,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "    val_dl   = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False,\n",
    "                          num_workers=cfg.num_workers, pin_memory=True)\n",
    "    return train_dl, val_dl\n",
    "\n",
    "train_dl, val_dl = build_dataloaders(cfg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb001eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Train / Validate / Test\n",
    "\n",
    "def train_one_epoch(model, dl, opt, epoch, cfg: CFG):\n",
    "    model.train()\n",
    "    total = 0.0\n",
    "    for imgs, _ in dl:\n",
    "        imgs = imgs.to(device)\n",
    "        loss, pred, idx, target = model(imgs)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item() * imgs.size(0)\n",
    "    return total / len(dl.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, dl, cfg: CFG, save_samples=False):\n",
    "    model.eval()\n",
    "    total = 0.0\n",
    "    for i, (imgs, _) in enumerate(dl):\n",
    "        imgs = imgs.to(device)\n",
    "        loss, pred, (keep_ids, mask_ids), target = model(imgs)\n",
    "        total += loss.item() * imgs.size(0)\n",
    "        if save_samples and i == 0:\n",
    "            save_grid(imgs[:16].cpu(), f\"{cfg.save_dir}/viz/input_epoch.jpg\", nrow=4)\n",
    "    return total / len(dl.dataset)\n",
    "\n",
    "@torch.no_grad()\n",
    "def test_reconstruction(model, dl, cfg: CFG):\n",
    "    model.eval()\n",
    "    te_loss = None\n",
    "    for i, (imgs, _) in enumerate(dl):\n",
    "        imgs = imgs.to(device)\n",
    "        loss, pred, (keep_ids, mask_ids), target = model(imgs)\n",
    "        te_loss = loss.item()\n",
    "        # 데모에선 입력만 저장 (전체 복원 이미지는 ids_restore 로직이 필요)\n",
    "        if i == 0:\n",
    "            save_grid(imgs[:16].cpu(), f\"{cfg.save_dir}/viz/test_input.jpg\", nrow=4)\n",
    "            break\n",
    "    return float(te_loss) if te_loss is not None else float(\"nan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d27e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Inference & Visualization Preview\n",
    "\n",
    "model = MAE(cfg).to(device)\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)\n",
    "sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)\n",
    "\n",
    "best_val = 1e9\n",
    "for epoch in range(1, cfg.epochs+1):\n",
    "    tr = train_one_epoch(model, train_dl, opt, epoch, cfg)\n",
    "    va = validate(model, val_dl, cfg, save_samples=(epoch % 1 == 0))\n",
    "    sch.step()\n",
    "    print(f\"[{epoch:03d}] train {tr:.4f} | val {va:.4f}\")\n",
    "    if va < best_val:\n",
    "        best_val = va\n",
    "        torch.save(model.state_dict(), f\"{cfg.save_dir}/ckpt/mae_best.pth\")\n",
    "\n",
    "te = test_reconstruction(model, val_dl, cfg)\n",
    "print(f\"[DONE] best_val={best_val:.4f} | test_recon_loss={te:.4f}\")\n",
    "print(f\"[ARTIFACTS] {Path(cfg.save_dir).resolve()}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mae-from-scratch)",
   "language": "python",
   "name": "mae-from-scratch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
