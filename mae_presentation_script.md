# MAE (Masked Autoencoder) êµ¬í˜„ ë°œí‘œ ëŒ€ë³¸

---

## ğŸ“Œ ë„ì…ë¶€

ì•ˆë…•í•˜ì„¸ìš”. ì˜¤ëŠ˜ì€ **Masked Autoencoder (MAE)** ë¥¼ PyTorchë¡œ ì²˜ìŒë¶€í„° êµ¬í˜„í•œ ì½”ë“œë¥¼ ë°œí‘œí•˜ê² ìŠµë‹ˆë‹¤.

MAEëŠ” 2021ë…„ Meta AIì—ì„œ ë°œí‘œí•œ Self-Supervised Learning ë°©ë²•ìœ¼ë¡œ, ì´ë¯¸ì§€ì˜ ì¼ë¶€ë¥¼ ê°€ë¦¬ê³  ë³µì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. BERTì˜ masking ì•„ì´ë””ì–´ë¥¼ ë¹„ì „ì— ì ìš©í•œ ê²ƒì¸ë°, ë†€ëê²Œë„ 75%ì˜ íŒ¨ì¹˜ë¥¼ ê°€ë ¤ë„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì˜¤ëŠ˜ ë°œí‘œëŠ” í¬ê²Œ 8ê°œ ì„¹ì…˜ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:
1. Setup & Imports
2. Configuration
3. Utility Functions
4. ViT Token Extractor
5. MAE Model (í•µì‹¬)
6. Dataset & DataLoader
7. Training/Validation/Test
8. Complete Pipeline

---

## 1ï¸âƒ£ Setup & Imports

```python
import torch
import torch.nn as nn
import timm  # ViT backbone
from torchvision import transforms, datasets
```

ë¨¼ì € í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ importí•©ë‹ˆë‹¤. 
- **PyTorch**: ê¸°ë³¸ ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- **timm**: Vision Transformer(ViT) ë°±ë³¸ì„ ì‰½ê²Œ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ ì‚¬ìš©
- **torchvision**: ë°ì´í„° ë¡œë”©ê³¼ ì „ì²˜ë¦¬ìš©

```python
def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
```

ì¬í˜„ì„±ì„ ìœ„í•´ ëª¨ë“  ëœë¤ ì‹œë“œë¥¼ ê³ ì •í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•  ê²½ìš° CUDA ì‹œë“œë„ í•¨ê»˜ ì„¤ì •í•©ë‹ˆë‹¤.

```python
device = "cuda" if torch.cuda.is_available() else "cpu"
```

GPUê°€ ìˆìœ¼ë©´ cudaë¥¼, ì—†ìœ¼ë©´ cpuë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.

---

## 2ï¸âƒ£ Configuration

```python
@dataclass
class CFG:
    img_size: int = 224
    patch_size: int = 16
    mask_ratio: float = 0.75
```

ì„¤ì •ê°’ë“¤ì„ dataclassë¡œ ê´€ë¦¬í•©ë‹ˆë‹¤.
- **img_size**: ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸° (224x224)
- **patch_size**: í•œ íŒ¨ì¹˜ì˜ í¬ê¸° (16x16) â†’ ì´ 196ê°œì˜ íŒ¨ì¹˜ê°€ ìƒì„±ë©ë‹ˆë‹¤
- **mask_ratio**: ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ (0.75 = 75%) â†’ MAEì˜ í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°

```python
enc_name: str = "vit_base_patch16_224"
dec_dim: int = 384
dec_depth: int = 6
```

- **enc_name**: timmì—ì„œ ì œê³µí•˜ëŠ” ViT Base ëª¨ë¸ì„ ì¸ì½”ë”ë¡œ ì‚¬ìš©
- **dec_dim**: ë””ì½”ë”ì˜ hidden dimension (384) - ì¸ì½”ë”(768)ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤
- **dec_depth**: ë””ì½”ë”ì˜ Transformer ë ˆì´ì–´ ìˆ˜ (6ê°œ) - ì¸ì½”ë”(12ê°œ)ë³´ë‹¤ ì ìŠµë‹ˆë‹¤

ì´ê²ƒì´ MAEì˜ **Asymmetric Encoder-Decoder** êµ¬ì¡°ì…ë‹ˆë‹¤. ì¸ì½”ë”ëŠ” í¬ê³  ê¹Šê²Œ, ë””ì½”ë”ëŠ” ê°€ë³ê²Œ ë§Œë“¤ì–´ì„œ íš¨ìœ¨ì„±ì„ ë†’ì…ë‹ˆë‹¤.

---

## 3ï¸âƒ£ Utility Functions

### Patchify - ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë¶„í•´

```python
class Patchify(nn.Module):
    def forward(self, imgs):  # (B,C,H,W) -> (B, N, P*P*C)
        x = imgs.reshape(B, C, h, p, w, p).permute(0, 2, 4, 3, 5, 1)
        return x.reshape(B, h*w, p*p*C)
```

ì…ë ¥ ì´ë¯¸ì§€ (B, 3, 224, 224)ë¥¼ íŒ¨ì¹˜ ì‹œí€€ìŠ¤ (B, 196, 768)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
- 224Ã·16 = 14, ë”°ë¼ì„œ 14Ã—14 = 196ê°œì˜ íŒ¨ì¹˜
- ê° íŒ¨ì¹˜ëŠ” 16Ã—16Ã—3 = 768 ì°¨ì›ì˜ ë²¡í„°

### Unpatchify - íŒ¨ì¹˜ë¥¼ ë‹¤ì‹œ ì´ë¯¸ì§€ë¡œ ë³µì›

```python
class Unpatchify(nn.Module):
    def forward(self, x):  # (B,N,P*P*C) -> (B,C,H,W)
```

Patchifyì˜ ì—­ì—°ì‚°ì…ë‹ˆë‹¤. ë””ì½”ë”ê°€ ë³µì›í•œ íŒ¨ì¹˜ë“¤ì„ ë‹¤ì‹œ ì´ë¯¸ì§€ í˜•íƒœë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.

### random_mask_indices - ëœë¤ ë§ˆìŠ¤í‚¹

```python
def random_mask_indices(num_patches, mask_ratio=0.75):
    n_mask = int(num_patches * mask_ratio)
    ids = torch.randperm(num_patches)
    mask_ids = ids[:n_mask]  # ê°€ë¦´ íŒ¨ì¹˜
    keep_ids = ids[n_mask:]  # ë³´ì—¬ì¤„ íŒ¨ì¹˜
    return keep_ids, mask_ids
```

196ê°œ íŒ¨ì¹˜ ì¤‘ 75%(147ê°œ)ëŠ” mask_ids, 25%(49ê°œ)ëŠ” keep_idsë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
- **keep_ids**: ì¸ì½”ë”ì— ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°ˆ íŒ¨ì¹˜ (ë³´ì´ëŠ” ë¶€ë¶„)
- **mask_ids**: ë””ì½”ë”ê°€ ë³µì›í•´ì•¼ í•  íŒ¨ì¹˜ (ê°€ë ¤ì§„ ë¶€ë¶„)

---

## 4ï¸âƒ£ ViT Token Extractor

```python
def vit_tokens_from_timm(vit: nn.Module, imgs: torch.Tensor):
    x = vit.patch_embed(imgs)  # (B, N, D)
```

timmì˜ ViT ëª¨ë¸ì—ì„œ í† í° ì‹œí€€ìŠ¤ë¥¼ ì¶”ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜ì…ë‹ˆë‹¤.

```python
cls_token = vit.cls_token.expand(B, -1, -1)  # (B,1,D)
x = torch.cat((cls_token, x), dim=1)         # (B, N+1, D)
```

**CLS í† í°**ì„ ì¶”ê°€í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì „ì²´ì˜ ìš”ì•½ ì •ë³´ë¥¼ í•™ìŠµí•˜ëŠ” íŠ¹ìˆ˜ í† í°ì…ë‹ˆë‹¤.

```python
x = x + vit.pos_embed
```

**Position Embedding**ì„ ì¶”ê°€í•©ë‹ˆë‹¤. TransformerëŠ” ìˆœì„œ ì •ë³´ê°€ ì—†ê¸° ë•Œë¬¸ì—, ê° í† í°ì´ ì´ë¯¸ì§€ì˜ ì–´ëŠ ìœ„ì¹˜ íŒ¨ì¹˜ì¸ì§€ ì•Œë ¤ì¤˜ì•¼ í•©ë‹ˆë‹¤.

```python
for blk in vit.blocks:
    x = blk(x)
x = vit.norm(x)
return x  # (B, N+1, D)
```

ViTì˜ Transformer ë¸”ë¡ë“¤ì„ í†µê³¼ì‹œí‚¨ í›„, Layer Normalizationì„ ì ìš©í•˜ê³  ë°˜í™˜í•©ë‹ˆë‹¤.

---

## 5ï¸âƒ£ MAE Model - í•µì‹¬ êµ¬í˜„

ì, ì´ì œ ê°€ì¥ ì¤‘ìš”í•œ MAE ëª¨ë¸ êµ¬í˜„ì…ë‹ˆë‹¤.

### __init__ - ëª¨ë¸ êµ¬ì¡° ì •ì˜

```python
class MAE(nn.Module):
    def __init__(self, cfg: CFG):
        # Encoder (timm ViT)
        self.encoder = timm.create_model(cfg.enc_name, pretrained=False)
        emb_dim = self.encoder.embed_dim  # 768
```

**ì¸ì½”ë”**ëŠ” timmì˜ ViT Baseë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë§ˆìŠ¤í¬ë˜ì§€ ì•Šì€ íŒ¨ì¹˜ë“¤ë§Œ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.

```python
        # Decoder
        self.mask_token = nn.Parameter(torch.zeros(1, 1, cfg.dec_dim))
```

**mask_token**: ë§ˆìŠ¤í¬ëœ ìœ„ì¹˜ë¥¼ ëŒ€ì²´í•  í•™ìŠµ ê°€ëŠ¥í•œ í† í°ì…ë‹ˆë‹¤. ë””ì½”ë”ëŠ” ì´ í† í°ì„ ë³´ê³  ì›ë˜ íŒ¨ì¹˜ë¥¼ ë³µì›í•´ì•¼ í•©ë‹ˆë‹¤.

```python
        self.dec_pos = nn.Parameter(torch.zeros(1, total_tokens, cfg.dec_dim))
```

ë””ì½”ë”ìš© **Position Embedding**ì…ë‹ˆë‹¤. ì¸ì½”ë”ì™€ ë³„ë„ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

```python
        self.enc_to_dec = nn.Linear(emb_dim, cfg.dec_dim)
```

ì¸ì½”ë”ì˜ ì¶œë ¥(768ì°¨ì›)ì„ ë””ì½”ë”ì˜ ì…ë ¥(384ì°¨ì›)ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” projection layerì…ë‹ˆë‹¤.

```python
        layer = nn.TransformerEncoderLayer(d_model=cfg.dec_dim, nhead=nhead, batch_first=True)
        self.decoder = nn.TransformerEncoder(layer, num_layers=cfg.dec_depth)
```

**ë””ì½”ë”**ëŠ” ê°€ë²¼ìš´ Transformerì…ë‹ˆë‹¤. 6ê°œ ë ˆì´ì–´, 384 ì°¨ì›ìœ¼ë¡œ ì¸ì½”ë”(12ë ˆì´ì–´, 768ì°¨ì›)ë³´ë‹¤ í›¨ì”¬ ì‘ìŠµë‹ˆë‹¤.

```python
        self.head = nn.Linear(cfg.dec_dim, cfg.patch_size * cfg.patch_size * 3)
```

ìµœì¢… ì¶œë ¥ headì…ë‹ˆë‹¤. 384ì°¨ì›ì„ 16Ã—16Ã—3 = 768 í”½ì…€ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

### forward - ìˆœì „íŒŒ ë¡œì§

```python
    def forward(self, imgs: torch.Tensor):
        # 1) target patches
        target = self.patchify(imgs)  # (B, N, P2*C)
```

ë¨¼ì € ì…ë ¥ ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. ì´ê²ƒì´ ìš°ë¦¬ê°€ ë³µì›í•´ì•¼ í•  **ì •ë‹µ(target)**ì…ë‹ˆë‹¤.

```python
        # 2) mask indices per-sample
        keep_ids, mask_ids = [], []
        for _ in range(B):
            k, m = random_mask_indices(N, self.cfg.mask_ratio)
            keep_ids.append(k); mask_ids.append(m)
```

ë°°ì¹˜ì˜ ê° ìƒ˜í”Œë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ëœë¤ ë§ˆìŠ¤í‚¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
- **keep_ids**: ì¸ì½”ë”ì— ë„£ì„ íŒ¨ì¹˜ (ë³´ì´ëŠ” 25%)
- **mask_ids**: ê°€ë¦´ íŒ¨ì¹˜ (ìˆ¨ê¸¸ 75%)

```python
        # 3) Encoder tokens
        enc_all = vit_tokens_from_timm(self.encoder, imgs)   # (B, N+1, De)
        enc_tokens = enc_all[:, 1:, :]                       # (B, N, De)
```

**ì¤‘ìš”í•œ ì **: ì¸ì½”ë”ëŠ” ì „ì²´ ì´ë¯¸ì§€ë¥¼ ë´…ë‹ˆë‹¤! 
ì‹¤ì œë¡œëŠ” keep ìœ„ì¹˜ë§Œ ì‚¬ìš©í•˜ì§€ë§Œ, ViT êµ¬ì¡°ìƒ ì „ì²´ ì´ë¯¸ì§€ë¥¼ ë„£ì–´ íŒ¨ì¹˜ ì„ë² ë”©ì„ ë§Œë“  í›„ í•„ìš”í•œ ë¶€ë¶„ë§Œ ì„ íƒí•©ë‹ˆë‹¤.

CLS í† í°(ì²« ë²ˆì§¸ í† í°)ì€ ì œì™¸í•˜ê³  íŒ¨ì¹˜ í† í°ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.

```python
        enc_kept = torch.gather(
            enc_tokens, dim=1,
            index=keep_ids.unsqueeze(-1).expand(-1, -1, enc_tokens.size(-1))
        )  # (B, Nk, De)
```

`torch.gather`ë¡œ keep_idsì— í•´ë‹¹í•˜ëŠ” í† í°ë§Œ ì„ íƒí•©ë‹ˆë‹¤. 
ì´ê²ƒì´ **ì¸ì½”ë”ê°€ ì‹¤ì œë¡œ ë³¸ í† í°ë“¤ì˜ latent representation**ì…ë‹ˆë‹¤.

```python
        # 4) Decoder input: kept + mask
        dec_kept = self.enc_to_dec(enc_kept)      # (B, Nk, Dd)
        dec_mask = self.mask_token.expand(B, Nm, -1)
        dec_in = torch.cat([dec_kept, dec_mask], dim=1) + self.dec_pos[:, :Nk+Nm, :]
```

ë””ì½”ë” ì…ë ¥ì„ êµ¬ì„±í•©ë‹ˆë‹¤:
1. **enc_kept**: ì¸ì½”ë”ê°€ ë³¸ íŒ¨ì¹˜ë“¤ (Projection í›„)
2. **mask_token**: ê°€ë ¤ì§„ íŒ¨ì¹˜ë“¤ì„ ëŒ€ì²´í•  í•™ìŠµ ê°€ëŠ¥í•œ í† í°
3. ë‘ ê°œë¥¼ concatenateí•˜ê³  position embedding ì¶”ê°€

ì´ê²Œ MAEì˜ í•µì‹¬ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤! ë””ì½”ë”ëŠ” ì¼ë¶€ ì‹¤ì œ ì •ë³´(kept)ì™€ mask tokenì„ ëª¨ë‘ ë°›ì•„ì„œ ë³µì›í•©ë‹ˆë‹¤.

```python
        dec_out = self.decoder(dec_in)            # (B, Nk+Nm, Dd)
        pred = self.head(dec_out[:, Nk:, :])      # (B, Nm, P2*C)
```

ë””ì½”ë”ë¥¼ í†µê³¼ì‹œí‚¨ í›„, **ë§ˆìŠ¤í¬ëœ ë¶€ë¶„ë§Œ** ì˜ˆì¸¡í•©ë‹ˆë‹¤ (`[:, Nk:]`).
kept ë¶€ë¶„ì€ ì´ë¯¸ ì •ë‹µì„ ì•„ë‹ˆê¹Œ loss ê³„ì‚°ì—ì„œ ì œì™¸í•©ë‹ˆë‹¤.

```python
        target_masked = torch.gather(
            target, dim=1,
            index=mask_ids.unsqueeze(-1).expand(-1, -1, target.size(-1))
        )
        loss = F.mse_loss(pred, target_masked)
```

ë§ˆìŠ¤í¬ëœ íŒ¨ì¹˜ì˜ ì •ë‹µ(target_masked)ê³¼ ì˜ˆì¸¡(pred)ì˜ MSE lossë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

**í”½ì…€ ë ˆë²¨ ë³µì›**ì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê° íŒ¨ì¹˜ì˜ 768ê°œ í”½ì…€ê°’ì„ ì •í™•íˆ ë§ì¶”ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.

---

## 6ï¸âƒ£ Dataset & DataLoader

```python
def build_dataloaders(cfg: CFG):
    tfm = transforms.Compose([
        transforms.Resize((cfg.img_size, cfg.img_size)),
        transforms.ToTensor()
    ])
```

ê°„ë‹¨í•œ ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰í•©ë‹ˆë‹¤:
- 224Ã—224ë¡œ resize
- Tensorë¡œ ë³€í™˜ (0~1 ì •ê·œí™” ìë™ ì ìš©)

**ì¤‘ìš”**: MAEëŠ” self-supervisedì´ë¯€ë¡œ ë ˆì´ë¸”ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤! ì´ë¯¸ì§€ ìì²´ê°€ ì…ë ¥ì´ì ì •ë‹µì…ë‹ˆë‹¤.

```python
    if train_dir.exists() and val_dir.exists():
        train_ds = datasets.ImageFolder(str(train_dir), transform=tfm)
    else:
        train_ds = FakeData(size=256, ...)
```

ì‹¤ì œ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ImageFolderë¡œ ë¡œë“œí•˜ê³ , ì—†ìœ¼ë©´ FakeDataë¡œ ë°ëª¨ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

---

## 7ï¸âƒ£ Training / Validation / Test

### train_one_epoch

```python
def train_one_epoch(model, dl, opt, epoch, cfg: CFG):
    model.train()
    for imgs, _ in dl:
        loss, pred, idx, target = model(imgs)
        opt.zero_grad(set_to_none=True)
        loss.backward()
        opt.step()
```

ì „í˜•ì ì¸ PyTorch í•™ìŠµ ë£¨í”„ì…ë‹ˆë‹¤.
- Forward passë¡œ loss ê³„ì‚°
- Backward passë¡œ gradient ê³„ì‚°
- Optimizerë¡œ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸

ë ˆì´ë¸”(\_)ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Self-supervised learningì˜ íŠ¹ì§•ì…ë‹ˆë‹¤.

### validate

```python
@torch.no_grad()
def validate(model, dl, cfg: CFG, save_samples=False):
    model.eval()
    # ... validation loss ê³„ì‚°
    if save_samples:
        save_grid(imgs[:16].cpu(), f"{cfg.save_dir}/viz/input_epoch.jpg")
```

Validation ì¤‘ì— ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì €ì¥í•´ì„œ ë‚˜ì¤‘ì— ë³µì› ê²°ê³¼ì™€ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 8ï¸âƒ£ Complete Pipeline

```python
model = MAE(cfg).to(device)
opt = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.wd)
sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=cfg.epochs)
```

MAE ë…¼ë¬¸ì—ì„œ ê¶Œì¥í•˜ëŠ” ì„¤ì •:
- **AdamW** optimizer (weight decay í¬í•¨)
- **Cosine annealing** learning rate scheduler
- Learning rate: 1e-4, Weight decay: 0.05

```python
for epoch in range(1, cfg.epochs+1):
    tr = train_one_epoch(model, train_dl, opt, epoch, cfg)
    va = validate(model, val_dl, cfg, save_samples=(epoch % 1 == 0))
    sch.step()
```

ì „ì²´ í•™ìŠµ ë£¨í”„ì…ë‹ˆë‹¤:
1. í•œ ì—í­ í•™ìŠµ
2. Validation ìˆ˜í–‰
3. Learning rate ì¡°ì •
4. ë² ìŠ¤íŠ¸ ëª¨ë¸ ì €ì¥

---

## ğŸ¯ í•µì‹¬ ìš”ì•½

MAEì˜ í•µì‹¬ ì•„ì´ë””ì–´ 3ê°€ì§€:

### 1. High Masking Ratio (75%)
- BERTëŠ” 15% ë§ˆìŠ¤í‚¹ vs MAEëŠ” 75% ë§ˆìŠ¤í‚¹
- ë¹„ì „ì€ redundancyê°€ ë†’ì•„ì„œ ê°€ëŠ¥
- ê³„ì‚° íš¨ìœ¨ë„ 3ë°° í–¥ìƒ (ì¸ì½”ë”ê°€ 25%ë§Œ ì²˜ë¦¬)

### 2. Asymmetric Encoder-Decoder
- **Encoder**: í¬ê³  ê¹Šê²Œ (ViT-Base, 768dim, 12 layers)
  - ë³´ì´ëŠ” 25% íŒ¨ì¹˜ë¡œ ê°•ë ¥í•œ representation í•™ìŠµ
- **Decoder**: ì‘ê³  ê°€ë³ê²Œ (384dim, 6 layers)
  - ë³µì›ì€ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ìš´ ì‘ì—…
  - Pre-training í›„ ë²„ë ¤ì§ (downstream taskëŠ” encoderë§Œ ì‚¬ìš©)

### 3. Pixel-level Reconstruction
- Normalized pixel valuesë¥¼ ì§ì ‘ ì˜ˆì¸¡
- MSE lossë¡œ ê°„ë‹¨í•˜ê²Œ í•™ìŠµ
- ëŒ€ì¡° í•™ìŠµ(contrastive)ë³´ë‹¤ êµ¬í˜„ì´ ì‰½ê³  íš¨ê³¼ì 

---

## ğŸš€ ì‹¤ì œ ì‚¬ìš©ë²•

```python
# 1) Pre-training (ì´ ì½”ë“œ)
model = MAE(cfg).to(device)
# ... ëŒ€ìš©ëŸ‰ unlabeled ë°ì´í„°ë¡œ í•™ìŠµ

# 2) Fine-tuning (downstream task)
encoder = model.encoder  # í•™ìŠµëœ ì¸ì½”ë”ë§Œ ì¶”ì¶œ
classifier = nn.Linear(768, num_classes)  # Classification head ì¶”ê°€
# ... labeled ë°ì´í„°ë¡œ fine-tuning
```

MAEë¡œ pre-trainingí•œ ì¸ì½”ë”ëŠ” ë‹¤ì–‘í•œ downstream taskì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:
- Image Classification
- Object Detection
- Semantic Segmentation
- ë“±ë“±

---

## ğŸ“Š MAEì˜ ì¥ì 

1. **ë°ì´í„° íš¨ìœ¨ì„±**: ë ˆì´ë¸” ì—†ì´ í•™ìŠµ ê°€ëŠ¥
2. **ê³„ì‚° íš¨ìœ¨ì„±**: 25% íŒ¨ì¹˜ë§Œ ì¸ì½”ë”© â†’ 3ë°° ë¹ ë¦„
3. **í™•ì¥ì„±**: ëª¨ë¸ í¬ê¸°ë¥¼ í‚¤ìš¸ìˆ˜ë¡ ì„±ëŠ¥ í–¥ìƒ
4. **ë²”ìš©ì„±**: ë‹¤ì–‘í•œ downstream taskì— ì „ì´ ê°€ëŠ¥
5. **êµ¬í˜„ ê°„ë‹¨ì„±**: ëŒ€ì¡° í•™ìŠµë³´ë‹¤ êµ¬ì¡°ê°€ ë‹¨ìˆœ

---

## ğŸ¬ ë§ˆë¬´ë¦¬

ì˜¤ëŠ˜ ë°œí‘œì—ì„œëŠ” MAEì˜ ì „ì²´ êµ¬í˜„ì„ ë‹¨ê³„ë³„ë¡œ ì‚´í´ë´¤ìŠµë‹ˆë‹¤:

1. âœ… ì´ë¯¸ì§€ë¥¼ íŒ¨ì¹˜ë¡œ ë‚˜ëˆ„ê¸° (Patchify)
2. âœ… ëœë¤í•˜ê²Œ 75% ë§ˆìŠ¤í‚¹
3. âœ… ë³´ì´ëŠ” 25%ë§Œ ì¸ì½”ë”ì— ì…ë ¥
4. âœ… ë””ì½”ë”ë¡œ ê°€ë ¤ì§„ 75% ë³µì›
5. âœ… MSE lossë¡œ í•™ìŠµ

í•µì‹¬ì€ **"ì ê²Œ ë³´ê³ , ë§ì´ ë³µì›í•˜ê¸°"** ì…ë‹ˆë‹¤.

ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ ì‹¤ì œë¡œ MAEë¥¼ í•™ìŠµì‹œí‚¤ê³ , ë§ˆìŠ¤í¬ëœ ì´ë¯¸ì§€ë¥¼ ë³µì›í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì§ˆë¬¸ ìˆìœ¼ì‹œë©´ í¸í•˜ê²Œ í•´ì£¼ì„¸ìš”. ê°ì‚¬í•©ë‹ˆë‹¤! ğŸ™

---

## ğŸ“š ì°¸ê³ ìë£Œ

- **ë…¼ë¬¸**: "Masked Autoencoders Are Scalable Vision Learners" (He et al., CVPR 2022)
- **GitHub**: https://github.com/facebookresearch/mae
- **timm library**: https://github.com/huggingface/pytorch-image-models
